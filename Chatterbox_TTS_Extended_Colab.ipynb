{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# ğŸ§ Chatterbox-TTS-Extended on Google Colab\n",
    "\n",
    "## Advanced Text-to-Speech with Artifact Reduction\n",
    "\n",
    "This notebook allows you to run Chatterbox-TTS-Extended on Google Colab, leveraging free GPU resources for high-quality speech synthesis with built-in artifact reduction.\n",
    "\n",
    "### Features:\n",
    "- ğŸ¤ High-quality voice cloning and TTS\n",
    "- ğŸ”§ Advanced artifact reduction with RNNoise\n",
    "- ğŸ¯ Whisper-based quality validation\n",
    "- ğŸ¨ Voice conversion capabilities\n",
    "- ğŸ“¦ Multiple export formats (WAV, MP3, FLAC)\n",
    "\n",
    "### Requirements:\n",
    "- Google Colab account (free tier works!)\n",
    "- GPU runtime (recommended: T4 or better)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick-links"
   },
   "source": [
    "## ğŸ“š Quick Links & Resources\n",
    "\n",
    "**Documentation:**\n",
    "- ğŸ“– [Full Colab Guide](https://github.com/m-marie1/Chatterbox-TTS-Extended/blob/main/COLAB_GUIDE.md) - Comprehensive guide\n",
    "- âš¡ [Quick Reference](https://github.com/m-marie1/Chatterbox-TTS-Extended/blob/main/COLAB_QUICKREF.md) - Cheat sheet\n",
    "- ğŸ“‹ [README](https://github.com/m-marie1/Chatterbox-TTS-Extended/blob/main/README.md) - Feature documentation\n",
    "\n",
    "**Support:**\n",
    "- ğŸ› [Report Issues](https://github.com/m-marie1/Chatterbox-TTS-Extended/issues)\n",
    "- ğŸ’¬ [Discussions](https://github.com/m-marie1/Chatterbox-TTS-Extended/discussions)\n",
    "\n",
    "**Tips:**\n",
    "- âš¡ For fastest start: Run all cells and accept default settings\n",
    "- ğŸ¯ For best quality: Enable RNNoise + use 3 candidates + Whisper validation\n",
    "- ğŸ’¾ Remember to download files before session ends!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## ğŸ“‹ Step 1: Environment Setup\n",
    "\n",
    "**IMPORTANT**: Make sure you have enabled GPU runtime:\n",
    "1. Go to `Runtime` â†’ `Change runtime type`\n",
    "2. Select `GPU` as Hardware accelerator\n",
    "3. Choose `T4` GPU (or better if available)\n",
    "4. Click `Save`\n",
    "\n",
    "This cell will:\n",
    "- Check GPU availability\n",
    "- Verify Python version\n",
    "- Display system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "check-environment"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking environment...\n",
      "\n",
      "Python version: 3.12.12 (main, Oct 10 2025, 08:52:57) [GCC 11.4.0]\n",
      "Python executable: /usr/bin/python3\n",
      "\n",
      "âœ… GPU detected:\n",
      "Sun Dec 21 04:46:15 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   47C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check GPU and environment\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"ğŸ” Checking environment...\\n\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Check GPU\n",
    "try:\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi'], encoding='utf-8')\n",
    "    print(\"\\nâœ… GPU detected:\")\n",
    "    print(gpu_info)\n",
    "except:\n",
    "    print(\"\\nâš ï¸  WARNING: No GPU detected. This will be VERY slow!\")\n",
    "    print(\"Please enable GPU: Runtime â†’ Change runtime type â†’ GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-system-deps"
   },
   "source": [
    "## ğŸ“¦ Step 2: Install System Dependencies\n",
    "\n",
    "Installing FFmpeg and other system-level tools required for audio processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "install-ffmpeg"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install FFmpeg (required for audio processing)\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq ffmpeg\n",
    "\n",
    "# Verify FFmpeg installation\n",
    "!ffmpeg -version | head -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone-repo"
   },
   "source": [
    "## ğŸ“¥ Step 3: Clone Repository\n",
    "\n",
    "Cloning the Chatterbox-TTS-Extended repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "git-clone"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“¥ Cloning Chatterbox-TTS-Extended repository...\n",
      "Cloning into 'Chatterbox-TTS-Extended'...\n",
      "remote: Enumerating objects: 847, done.\u001b[K\n",
      "remote: Counting objects: 100% (224/224), done.\u001b[K\n",
      "remote: Compressing objects: 100% (117/117), done.\u001b[K\n",
      "remote: Total 847 (delta 164), reused 130 (delta 103), pack-reused 623 (from 2)\u001b[K\n",
      "Receiving objects: 100% (847/847), 1.36 MiB | 24.48 MiB/s, done.\n",
      "Resolving deltas: 100% (360/360), done.\n",
      "/content/Chatterbox-TTS-Extended\n",
      "\n",
      "âœ… Repository cloned successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists('Chatterbox-TTS-Extended'):\n",
    "    print(\"ğŸ“ Removing existing directory...\")\n",
    "    !rm -rf Chatterbox-TTS-Extended\n",
    "\n",
    "# Clone the repository\n",
    "print(\"ğŸ“¥ Cloning Chatterbox-TTS-Extended repository...\")\n",
    "!git clone https://github.com/m-marie1/Chatterbox-TTS-Extended.git\n",
    "\n",
    "# Change to repository directory\n",
    "%cd Chatterbox-TTS-Extended\n",
    "\n",
    "print(\"\\nâœ… Repository cloned successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-python-deps"
   },
   "source": [
    "## ğŸ Step 4: Install Python Dependencies\n",
    "\n",
    "Installing all required Python packages. This may take 3-5 minutes.\n",
    "\n",
    "**Note**: We're using Colab-optimized versions to avoid conflicts with pre-installed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Installing PyTorch with CUDA support...\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m798.9/798.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m105.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m105.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m111.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\n",
      "ğŸ“¦ Installing core dependencies...\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m40.5/40.5 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m130.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m99.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m252.9/252.9 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "ğŸ¬ Installing auto-editor...\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.5/105.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25h\n",
      "ğŸ¤— Installing Hugging Face dependencies...\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m124.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m107.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "ğŸ¯ Installing model-specific dependencies...\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m62.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h\n",
      "ğŸ”‡ Installing pyrnnoise for noise reduction...\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m99.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "ğŸ”¤ Installing s3tokenizer...\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m225.2/225.2 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.1/18.1 MB\u001b[0m \u001b[31m86.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m226.4/226.4 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.2/99.2 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for s3tokenizer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\n",
      "ğŸš€ Installing spaces...\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m105.7/105.7 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\n",
      "âœ… All dependencies installed successfully!\n",
      "\n",
      "ğŸ“Š Verifying installations...\n",
      "PyTorch version: 2.4.1+cu121\n",
      "CUDA available: True\n",
      "CUDA version: 12.1\n",
      "GPU device: Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# Install PyTorch with CUDA support (Colab uses CUDA 12.x)\n",
    "print(\"ğŸ”§ Installing PyTorch with CUDA support...\")\n",
    "!pip install -q torch==2.4.1 torchvision==0.19.1 torchaudio==2.4.1 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install core dependencies\n",
    "print(\"\\nğŸ“¦ Installing core dependencies...\")\n",
    "!pip install -q gradio numpy faster-whisper openai-whisper ffmpeg-python\n",
    "!pip install -q resampy==0.4.3 librosa==0.10.0 soundfile nltk\n",
    "\n",
    "# Install auto-editor for audio cleanup\n",
    "print(\"\\nğŸ¬ Installing auto-editor...\")\n",
    "!pip install -q auto-editor==27.1.1\n",
    "\n",
    "# Install Hugging Face and model dependencies\n",
    "print(\"\\nğŸ¤— Installing Hugging Face dependencies...\")\n",
    "!pip install -q transformers==4.46.3 diffusers==0.29.0 omegaconf==2.3.0\n",
    "\n",
    "# Install specific model dependencies\n",
    "print(\"\\nğŸ¯ Installing model-specific dependencies...\")\n",
    "!pip install -q resemble-perth==1.0.1 silero-vad==5.1.2 conformer==0.3.2\n",
    "\n",
    "# Install pyrnnoise for artifact reduction\n",
    "print(\"\\nğŸ”‡ Installing pyrnnoise for noise reduction...\")\n",
    "!pip install -q pyrnnoise==0.3.8\n",
    "\n",
    "# Install s3tokenizer\n",
    "print(\"\\nğŸ”¤ Installing s3tokenizer...\")\n",
    "!pip install -q s3tokenizer\n",
    "\n",
    "# Install spaces (for Gradio compatibility)\n",
    "print(\"\\nğŸš€ Installing spaces...\")\n",
    "!pip install -q spaces\n",
    "\n",
    "print(\"\\nâœ… All dependencies installed successfully!\")\n",
    "\n",
    "# Verify key installations\n",
    "print(\"\\nğŸ“Š Verifying installations...\")\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-nltk"
   },
   "source": [
    "## ğŸ“š Step 5: Download NLTK Data\n",
    "\n",
    "Downloading required NLTK tokenizer data for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "nltk-download"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“š Downloading NLTK data...\n",
      "âœ… NLTK data downloaded!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(\"ğŸ“š Downloading NLTK data...\")\n",
    "nltk.download('punkt_tab', quiet=True)\n",
    "print(\"âœ… NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "launch-ui"
   },
   "source": [
    "## ğŸš€ Step 6: Launch Chatterbox-TTS-Extended\n",
    "\n",
    "This will start the Gradio interface. The model will be loaded on first use.\n",
    "\n",
    "**Features available:**\n",
    "- **TTS Tab**: Text-to-Speech with advanced options\n",
    "  - Multiple candidate generation for best quality\n",
    "  - Whisper validation to reduce artifacts\n",
    "  - RNNoise denoising for clean audio\n",
    "  - Auto-editor for silence removal\n",
    "  - Batch processing support\n",
    "- **Voice Conversion Tab**: Convert voice to match a reference\n",
    "\n",
    "**Tips for Colab:**\n",
    "- First generation will take longer as models load\n",
    "- Use smaller Whisper models (tiny/base) to save VRAM\n",
    "- Enable \"Use faster-whisper\" for better performance\n",
    "- Start with 1-2 candidates per chunk to avoid OOM errors\n",
    "- If you get CUDA out of memory, restart runtime and reduce settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "launch-app",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Launching Chatterbox-TTS-Extended...\n",
      "\n",
      "â³ First generation will take longer as models download and load.\n",
      "ğŸ“Š Monitor the output below for progress updates.\n",
      "\n",
      "2025-12-21 04:52:30.803668: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1766292751.042514    4466 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1766292751.105931    4466 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1766292751.614525    4466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766292751.614570    4466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766292751.614574    4466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1766292751.614578    4466 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-21 04:52:31.668208: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "ğŸš€ Running on device: cuda\n",
      "Model not loaded, initializing...\n",
      "ve.safetensors: 100% 5.70M/5.70M [00:01<00:00, 4.04MB/s]\n",
      "t3_cfg.safetensors: 100% 2.13G/2.13G [00:19<00:00, 109MB/s] \n",
      "s3gen.safetensors: 100% 1.06G/1.06G [00:07<00:00, 149MB/s] \n",
      "tokenizer.json: 25.5kB [00:00, 86.2MB/s]\n",
      "conds.pt: 100% 107k/107k [00:00<00:00, 196kB/s]\n",
      "/usr/local/lib/python3.12/dist-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n",
      "Model loaded on device: cuda\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://98d2327c03740526bf.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "Multilingual model not loaded, initializing...\n",
      "Fetching 6 files:   0% 0/6 [00:00<?, ?it/s]\n",
      "Cangjie5_TC.json: 0.00B [00:00, ?B/s]\u001b[A\n",
      "\n",
      "grapheme_mtl_merged_expanded_v1.json: 70.0kB [00:00, 27.4MB/s]A\n",
      "Cangjie5_TC.json: 1.92MB [00:00, 65.5MB/s]\n",
      "Fetching 6 files:  17% 1/6 [00:00<00:01,  3.37it/s]\n",
      "s3gen.pt:   0% 0.00/1.06G [00:00<?, ?B/s]\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:   0% 0.00/2.14G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "ve.pt:   0% 0.00/5.70M [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "ve.pt: 100% 5.70M/5.70M [00:01<00:00, 3.49MB/s]\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:   0% 4.06M/2.14G [00:02<20:11, 1.77MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:   4% 75.1M/2.14G [00:09<03:58, 8.69MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:   7% 148M/2.14G [00:09<01:45, 18.9MB/s] \u001b[A\u001b[A\n",
      "s3gen.pt:   6% 67.1M/1.06G [00:10<02:38, 6.26MB/s]\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  10% 220M/2.14G [00:11<01:12, 26.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  14% 291M/2.14G [00:14<01:15, 24.6MB/s]\u001b[A\u001b[A\n",
      "s3gen.pt:  11% 118M/1.06G [00:14<01:49, 8.56MB/s] \u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  17% 369M/2.14G [00:15<00:50, 35.3MB/s]\u001b[A\u001b[A\n",
      "s3gen.pt:  18% 185M/1.06G [00:15<00:55, 15.6MB/s]\u001b[A\n",
      "s3gen.pt:  24% 252M/1.06G [00:15<00:32, 24.9MB/s]\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  21% 440M/2.14G [00:16<00:39, 42.8MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  24% 512M/2.14G [00:16<00:27, 59.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  27% 583M/2.14G [00:16<00:19, 81.8MB/s]\u001b[A\u001b[A\n",
      "s3gen.pt:  30% 319M/1.06G [00:16<00:22, 32.3MB/s]\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  30% 654M/2.14G [00:17<00:17, 86.2MB/s]\u001b[A\u001b[A\n",
      "s3gen.pt:  37% 387M/1.06G [00:17<00:16, 40.8MB/s]\u001b[A\n",
      "s3gen.pt:  43% 454M/1.06G [00:17<00:10, 57.1MB/s]\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  34% 725M/2.14G [00:20<00:32, 43.8MB/s]\u001b[A\u001b[A\n",
      "s3gen.pt:  49% 521M/1.06G [00:20<00:14, 37.4MB/s]\u001b[A\n",
      "s3gen.pt:  56% 588M/1.06G [00:24<00:16, 28.0MB/s]\u001b[A\n",
      "s3gen.pt:  68% 722M/1.06G [00:25<00:06, 49.9MB/s]\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  40% 867M/2.14G [00:30<00:58, 21.8MB/s]\u001b[A\u001b[A\n",
      "s3gen.pt:  75% 789M/1.06G [00:30<00:09, 27.0MB/s]\u001b[A\n",
      "s3gen.pt:  81% 856M/1.06G [00:31<00:05, 35.2MB/s]\u001b[A\n",
      "s3gen.pt:  87% 923M/1.06G [00:31<00:02, 46.2MB/s]\u001b[A\n",
      "s3gen.pt:  94% 990M/1.06G [00:31<00:01, 60.4MB/s]\u001b[A\n",
      "s3gen.pt: 100% 1.06G/1.06G [00:32<00:00, 32.6MB/s]\u001b[A\n",
      "Fetching 6 files:  67% 4/6 [00:32<00:17,  8.87s/it]\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  44% 938M/2.14G [00:34<00:57, 20.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  47% 1.01G/2.14G [00:34<00:41, 27.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  50% 1.08G/2.14G [00:35<00:29, 36.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  54% 1.15G/2.14G [00:35<00:20, 48.2MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  57% 1.22G/2.14G [00:35<00:14, 62.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  60% 1.29G/2.14G [00:36<00:10, 79.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  64% 1.37G/2.14G [00:36<00:07, 101MB/s] \u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  67% 1.44G/2.14G [00:37<00:08, 84.5MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  70% 1.51G/2.14G [00:41<00:14, 42.9MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  73% 1.58G/2.14G [00:41<00:09, 58.1MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  77% 1.65G/2.14G [00:41<00:06, 72.6MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  80% 1.72G/2.14G [00:41<00:04, 92.4MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  83% 1.79G/2.14G [00:42<00:03, 114MB/s] \u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  87% 1.86G/2.14G [00:42<00:02, 136MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  90% 1.93G/2.14G [00:42<00:01, 160MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  93% 2.00G/2.14G [00:43<00:00, 177MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors:  97% 2.07G/2.14G [00:43<00:00, 134MB/s]\u001b[A\u001b[A\n",
      "\n",
      "t3_mtl23ls_v2.safetensors: 100% 2.14G/2.14G [00:49<00:00, 43.5MB/s]\u001b[A\u001b[A\n",
      "Fetching 6 files: 100% 6/6 [00:49<00:00,  8.29s/it]\n",
      "/usr/local/lib/python3.12/dist-packages/diffusers/models/lora.py:393: FutureWarning: `LoRACompatibleLinear` is deprecated and will be removed in version 1.0.0. Use of `LoRACompatibleLinear` is deprecated. Please switch to PEFT backend by installing PEFT: `pip install peft`.\n",
      "  deprecate(\"LoRACompatibleLinear\", \"1.0.0\", deprecation_message)\n",
      "Cangjie5_TC.json: 1.92MB [00:00, 49.5MB/s]\n",
      "WARNING:chatterbox.models.tokenizers.tokenizer:pkuseg not available - Chinese segmentation will be skipped\n",
      "/usr/local/lib/python3.12/dist-packages/perth/perth_net/perth_net_implicit/checkpoint_manager.py:36: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(ckpts[-1], map_location=\"cpu\")\n",
      "loaded PerthNet (Implicit) at step 250,000\n",
      "Multilingual model loaded on device: cuda\n",
      "[DEBUG] After reference number removal: 'Oh Mann... Max, hÃ¶rst du das? Das ist mein Magen. Er singt das Lied seines Volkes. Und es ist eine sehr, sehr traurige Ballade.'\n",
      "\u001b[32m[DEBUG] Split text into 4 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=29, content='\u001b[33mOh Mann... Max, hÃ¶rst du das?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Oh Mann... Max, hÃ¶rst du das?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=19, content='\u001b[33mDas ist mein Magen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist mein Magen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=32, content='\u001b[33mEr singt das Lied seines Volkes....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Er singt das Lied seines Volkes....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=44, content='\u001b[33mUnd es ist eine sehr, sehr traurige Ballade....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und es ist eine sehr, sehr traurige Ballade....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Oh Mann... Max, hÃ¶rst du das? Das ist mein Magen. Er singt das Lied seines Volkes. Und es ist eine sehr, sehr traurige Ballade....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=127, content='\u001b[33mOh Mann... Max, hÃ¶rst du das? Das ist mein Magen. Er singt das Lied seines Volkes. Und es ist eine sehr, sehr traurige Ballade....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 793552616\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=127:\u001b[33m Oh Mann... Max, hÃ¶rst du das? Das ist mein Magen. Er singt das Lied seines Volkes. Und es ist eine sehr, sehr traurige Ballade.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3954147000).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "LlamaModel is using LlamaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class (https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\n",
      "Sampling:  20% 201/1000 [00:13<00:47, 16.76it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  20% 201/1000 [00:13<00:54, 14.73it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=8.040s: temp/gen1_chunk_000_cand_1_try1_seed3954147000.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3954147000.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_050422_498_gen1_seed793552616.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.72 seconds (0:00:01)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_050422_498_gen1_seed793552616_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_050422_498_gen1_seed793552616.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_050422_498_gen1_seed793552616.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_050422_498_gen1_seed793552616.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_050422_498_gen1_seed793552616.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Ich hÃ¶re es bis nach Hamburg, Anna. Das ist ja ein ausgewachsenes Magen-Orchester. Hast du heute noch nichts gegessen oder was ist los?'\n",
      "\u001b[32m[DEBUG] Split text into 3 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=35, content='\u001b[33mIch hÃ¶re es bis nach Hamburg, Anna....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ich hÃ¶re es bis nach Hamburg, Anna....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=46, content='\u001b[33mDas ist ja ein ausgewachsenes Magen-Orchester....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist ja ein ausgewachsenes Magen-Orchester....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=52, content='\u001b[33mHast du heute noch nichts gegessen oder was ist los?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Hast du heute noch nichts gegessen oder was ist los?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Ich hÃ¶re es bis nach Hamburg, Anna. Das ist ja ein ausgewachsenes Magen-Orchester. Hast du heute noch nichts gegessen oder was ist los?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=135, content='\u001b[33mIch hÃ¶re es bis nach Hamburg, Anna. Das ist ja ein ausgewachsenes Magen-Orchester. Hast du heute noch nichts gegessen oder was ist los?...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2365271805\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=135:\u001b[33m Ich hÃ¶re es bis nach Hamburg, Anna. Das ist ja ein ausgewachsenes Magen-Orchester. Hast du heute noch nichts gegessen oder was ist los?\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=1756202551).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  23% 229/1000 [00:15<01:18,  9.86it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  23% 230/1000 [00:16<00:53, 14.33it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=9.200s: temp/gen1_chunk_000_cand_1_try1_seed1756202551.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed1756202551.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_050502_534_gen1_seed2365271805.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.24 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_050502_534_gen1_seed2365271805_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_050502_534_gen1_seed2365271805.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_050502_534_gen1_seed2365271805.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_050502_534_gen1_seed2365271805.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_050502_534_gen1_seed2365271805.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Doch, aber das ist Stunden her! Ich hatte so ein trauriges belegtes BrÃ¶tchen heute Mittag... weiÃŸt du, so eines aus der BÃ¤ckerei-Kette, wo der KÃ¤se schon so leicht schwitzt und das Salatblatt aussieht, als hÃ¤tte es seinen Lebenswillen verloren. Das zÃ¤hlt nicht. Das war nur... Existenzsicherung. Ich hab richtigen Hunger. So richtigen, ehrlichen Kohldampf.'\n",
      "\u001b[32m[DEBUG] Split text into 6 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=31, content='\u001b[33mDoch, aber das ist Stunden her!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Doch, aber das ist Stunden her!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=212, content='\u001b[33mIch hatte so ein trauriges belegtes BrÃ¶tchen heute Mittag... weiÃŸt du, so eines aus der BÃ¤ckerei-Kette, wo der KÃ¤se schon so leicht schwitzt und das Salatblatt aussieht, als hÃ¤tte es seinen Lebenswillen verloren....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ich hatte so ein trauriges belegtes BrÃ¶tchen heute Mittag... weiÃŸt du, so eines aus der BÃ¤ckerei-Kette, wo der KÃ¤se schon so leicht schwitzt und das Salatblatt aussieht, als hÃ¤tte es seinen Lebenswillen verloren....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=16, content='\u001b[33mDas zÃ¤hlt nicht....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das zÃ¤hlt nicht....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=33, content='\u001b[33mDas war nur... Existenzsicherung....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das war nur... Existenzsicherung....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=25, content='\u001b[33mIch hab richtigen Hunger....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Doch, aber das ist Stunden her! Ich hatte so ein trauriges belegtes BrÃ¶tchen heute Mittag... weiÃŸt du, so eines aus der BÃ¤ckerei-Kette, wo der KÃ¤se schon so leicht schwitzt und das Salatblatt aussieht, als hÃ¤tte es seinen Lebenswillen verloren. Das zÃ¤hlt nicht. Das war nur... Existenzsicherung....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Ich hab richtigen Hunger....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=34, content='\u001b[33mSo richtigen, ehrlichen Kohldampf....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: So richtigen, ehrlichen Kohldampf....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Ich hab richtigen Hunger. So richtigen, ehrlichen Kohldampf....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=295, content='\u001b[33mDoch, aber das ist Stunden her! Ich hatte so ein trauriges belegtes BrÃ¶tchen heute Mittag... weiÃŸt du, so eines aus der BÃ¤ckerei-Kette, wo der KÃ¤se schon so leicht schwitzt und das Salatblatt aussieht, als hÃ¤tte es seinen Lebenswillen verloren. Das zÃ¤hlt nicht. Das war nur... Existenzsicherung....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=60, content='\u001b[33mIch hab richtigen Hunger. So richtigen, ehrlichen Kohldampf....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 414478669\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=295:\u001b[33m Doch, aber das ist Stunden her! Ich hatte so ein trauriges belegtes BrÃ¶tchen heute Mittag... weiÃŸt du, so eines aus der BÃ¤ckerei-Kette, wo der KÃ¤se schon so leicht schwitzt und das Salatblatt aussieht, als hÃ¤tte es seinen Lebenswillen verloren. Das zÃ¤hlt nicht. Das war nur... Existenzsicherung.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2683470119).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=60:\u001b[33m Ich hab richtigen Hunger. So richtigen, ehrlichen Kohldampf.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=2683480126).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  41% 410/1000 [00:27<00:37, 15.53it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  41% 410/1000 [00:27<00:39, 14.76it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=16.400s: temp/gen1_chunk_000_cand_1_try1_seed2683470119.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  12% 116/1000 [00:07<00:57, 15.35it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  12% 116/1000 [00:07<00:57, 15.44it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=4.640s: temp/gen1_chunk_001_cand_1_try1_seed2683480126.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2683470119.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed2683480126.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_050559_140_gen1_seed414478669.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.58 seconds (0:00:01)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_050559_140_gen1_seed414478669_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_050559_140_gen1_seed414478669.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_050559_140_gen1_seed414478669.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_050559_140_gen1_seed414478669.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_050559_140_gen1_seed414478669.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Oh, den kenne ich. Der \"trauriges-BÃ¼ro-Mittagessen-Hunger\". Der ist besonders fies, weil er nicht nur physisch ist, sondern auch seelisch. Man fÃ¼hlt sich betrogen vom Leben.'\n",
      "\u001b[32m[DEBUG] Split text into 4 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=18, content='\u001b[33mOh, den kenne ich....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Oh, den kenne ich....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=40, content='\u001b[33mDer \"trauriges-BÃ¼ro-Mittagessen-Hunger\"....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Der \"trauriges-BÃ¼ro-Mittagessen-Hunger\"....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=78, content='\u001b[33mDer ist besonders fies, weil er nicht nur physisch ist, sondern auch seelisch....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Der ist besonders fies, weil er nicht nur physisch ist, sondern auch seelisch....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=34, content='\u001b[33mMan fÃ¼hlt sich betrogen vom Leben....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Man fÃ¼hlt sich betrogen vom Leben....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Oh, den kenne ich. Der \"trauriges-BÃ¼ro-Mittagessen-Hunger\". Der ist besonders fies, weil er nicht nur physisch ist, sondern auch seelisch. Man fÃ¼hlt sich betrogen vom Leben....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=173, content='\u001b[33mOh, den kenne ich. Der \"trauriges-BÃ¼ro-Mittagessen-Hunger\". Der ist besonders fies, weil er nicht nur physisch ist, sondern auch seelisch. Man fÃ¼hlt sich betrogen vom Leben....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3049829879\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=173:\u001b[33m Oh, den kenne ich. Der \"trauriges-BÃ¼ro-Mittagessen-Hunger\". Der ist besonders fies, weil er nicht nur physisch ist, sondern auch seelisch. Man fÃ¼hlt sich betrogen vom Leben.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=4226436517).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  29% 294/1000 [00:20<01:01, 11.47it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  29% 294/1000 [00:20<00:48, 14.51it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=11.760s: temp/gen1_chunk_000_cand_1_try1_seed4226436517.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed4226436517.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_050639_097_gen1_seed3049829879.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.26 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_050639_097_gen1_seed3049829879_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_050639_097_gen1_seed3049829879.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_050639_097_gen1_seed3049829879.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_050639_097_gen1_seed3049829879.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_050639_097_gen1_seed3049829879.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Genau! Betrogen! Das ist das perfekte Wort. Und immer wenn ich diesen Hunger hab, hab ich sofort ein ganz bestimmtes Bild im Kopf. Eine ganz bestimmte Rettung. Ein Leuchtfeuer in der Dunkelheit der kulinarischen Trostlosigkeit.'\n",
      "\u001b[32m[DEBUG] Split text into 6 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=6, content='\u001b[33mGenau!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Genau!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=9, content='\u001b[33mBetrogen!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Betrogen!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=26, content='\u001b[33mDas ist das perfekte Wort....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist das perfekte Wort....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=86, content='\u001b[33mUnd immer wenn ich diesen Hunger hab, hab ich sofort ein ganz bestimmtes Bild im Kopf....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und immer wenn ich diesen Hunger hab, hab ich sofort ein ganz bestimmtes Bild im Kopf....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=28, content='\u001b[33mEine ganz bestimmte Rettung....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Eine ganz bestimmte Rettung....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=67, content='\u001b[33mEin Leuchtfeuer in der Dunkelheit der kulinarischen Trostlosigkeit....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ein Leuchtfeuer in der Dunkelheit der kulinarischen Trostlosigkeit....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Genau! Betrogen! Das ist das perfekte Wort. Und immer wenn ich diesen Hunger hab, hab ich sofort ein ganz bestimmtes Bild im Kopf. Eine ganz bestimmte Rettung. Ein Leuchtfeuer in der Dunkelheit der kulinarischen Trostlosigkeit....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=227, content='\u001b[33mGenau! Betrogen! Das ist das perfekte Wort. Und immer wenn ich diesen Hunger hab, hab ich sofort ein ganz bestimmtes Bild im Kopf. Eine ganz bestimmte Rettung. Ein Leuchtfeuer in der Dunkelheit der kulinarischen Trostlosigkeit....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2505764007\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=227:\u001b[33m Genau! Betrogen! Das ist das perfekte Wort. Und immer wenn ich diesen Hunger hab, hab ich sofort ein ganz bestimmtes Bild im Kopf. Eine ganz bestimmte Rettung. Ein Leuchtfeuer in der Dunkelheit der kulinarischen Trostlosigkeit.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=1704459701).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  32% 320/1000 [00:21<00:42, 16.04it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  32% 320/1000 [00:21<00:46, 14.63it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=12.800s: temp/gen1_chunk_000_cand_1_try1_seed1704459701.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed1704459701.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_050719_221_gen1_seed2505764007.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.26 seconds (0:00:00)                                         AM    o volume |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 24.7%  ETA 05:07 AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_050719_221_gen1_seed2505764007_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_050719_221_gen1_seed2505764007.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_050719_221_gen1_seed2505764007.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_050719_221_gen1_seed2505764007.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_050719_221_gen1_seed2505764007.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Lass mich raten. Es ist rund, es ist warm, es riecht nach... Abenteuer und Knoblauch?'\n",
      "\u001b[32m[DEBUG] Split text into 2 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=16, content='\u001b[33mLass mich raten....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Lass mich raten....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=68, content='\u001b[33mEs ist rund, es ist warm, es riecht nach... Abenteuer und Knoblauch?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Es ist rund, es ist warm, es riecht nach... Abenteuer und Knoblauch?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Lass mich raten. Es ist rund, es ist warm, es riecht nach... Abenteuer und Knoblauch?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=85, content='\u001b[33mLass mich raten. Es ist rund, es ist warm, es riecht nach... Abenteuer und Knoblauch?...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1973098752\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=85:\u001b[33m Lass mich raten. Es ist rund, es ist warm, es riecht nach... Abenteuer und Knoblauch?\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=990481152).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  14% 143/1000 [00:09<00:54, 15.84it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  14% 143/1000 [00:09<00:57, 14.81it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=5.720s: temp/gen1_chunk_000_cand_1_try1_seed990481152.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed990481152.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_050745_008_gen1_seed1973098752.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.2 seconds (0:00:00)                                          AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_050745_008_gen1_seed1973098752_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_050745_008_gen1_seed1973098752.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_050745_008_gen1_seed1973098752.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_050745_008_gen1_seed1973098752.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_050745_008_gen1_seed1973098752.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Ja! Es riecht nach GlÃ¼ck! Also, fÃ¼r mich ist die Antwort auf die Frage \"Was essen wir?\" in 80 Prozent der FÃ¤lle: DÃ¶ner. Einfach DÃ¶ner. Es ist die ultimative Antwort.'\n",
      "\u001b[32m[DEBUG] Split text into 6 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=3, content='\u001b[33mJa!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ja!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=21, content='\u001b[33mEs riecht nach GlÃ¼ck!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Es riecht nach GlÃ¼ck!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=61, content='\u001b[33mAlso, fÃ¼r mich ist die Antwort auf die Frage \"Was essen wir?\"...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Also, fÃ¼r mich ist die Antwort auf die Frage \"Was essen wir?\"...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=31, content='\u001b[33min 80 Prozent der FÃ¤lle: DÃ¶ner....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: in 80 Prozent der FÃ¤lle: DÃ¶ner....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=14, content='\u001b[33mEinfach DÃ¶ner....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Einfach DÃ¶ner....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=30, content='\u001b[33mEs ist die ultimative Antwort....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Es ist die ultimative Antwort....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Ja! Es riecht nach GlÃ¼ck! Also, fÃ¼r mich ist die Antwort auf die Frage \"Was essen wir?\" in 80 Prozent der FÃ¤lle: DÃ¶ner. Einfach DÃ¶ner. Es ist die ultimative Antwort....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=165, content='\u001b[33mJa! Es riecht nach GlÃ¼ck! Also, fÃ¼r mich ist die Antwort auf die Frage \"Was essen wir?\" in 80 Prozent der FÃ¤lle: DÃ¶ner. Einfach DÃ¶ner. Es ist die ultimative Antwort....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2483516343\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=165:\u001b[33m Ja! Es riecht nach GlÃ¼ck! Also, fÃ¼r mich ist die Antwort auf die Frage \"Was essen wir?\" in 80 Prozent der FÃ¤lle: DÃ¶ner. Einfach DÃ¶ner. Es ist die ultimative Antwort.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=1904309989).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  26% 263/1000 [00:17<00:45, 16.09it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  26% 264/1000 [00:17<00:48, 15.09it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=10.560s: temp/gen1_chunk_000_cand_1_try1_seed1904309989.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed1904309989.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_050818_978_gen1_seed2483516343.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.29 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_050818_978_gen1_seed2483516343_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_050818_978_gen1_seed2483516343.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_050818_978_gen1_seed2483516343.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_050818_978_gen1_seed2483516343.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_050818_978_gen1_seed2483516343.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Ah, da sagst du was. DÃ¶ner. Das ist... ja, das ist mehr als nur ein schnelles Essen, oder? Das ist eine Institution. Ich wÃ¼rde fast sagen, das inoffizielle deutsche Nationalgericht.'\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=20, content='\u001b[33mAh, da sagst du was....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ah, da sagst du was....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=6, content='\u001b[33mDÃ¶ner....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: DÃ¶ner....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=62, content='\u001b[33mDas ist... ja, das ist mehr als nur ein schnelles Essen, oder?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist... ja, das ist mehr als nur ein schnelles Essen, oder?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=25, content='\u001b[33mDas ist eine Institution....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist eine Institution....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=64, content='\u001b[33mIch wÃ¼rde fast sagen, das inoffizielle deutsche Nationalgericht....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ich wÃ¼rde fast sagen, das inoffizielle deutsche Nationalgericht....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Ah, da sagst du was. DÃ¶ner. Das ist... ja, das ist mehr als nur ein schnelles Essen, oder? Das ist eine Institution. Ich wÃ¼rde fast sagen, das inoffizielle deutsche Nationalgericht....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=181, content='\u001b[33mAh, da sagst du was. DÃ¶ner. Das ist... ja, das ist mehr als nur ein schnelles Essen, oder? Das ist eine Institution. Ich wÃ¼rde fast sagen, das inoffizielle deutsche Nationalgericht....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1058100850\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=181:\u001b[33m Ah, da sagst du was. DÃ¶ner. Das ist... ja, das ist mehr als nur ein schnelles Essen, oder? Das ist eine Institution. Ich wÃ¼rde fast sagen, das inoffizielle deutsche Nationalgericht.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=176227286).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  32% 317/1000 [00:20<00:44, 15.45it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  32% 317/1000 [00:20<00:45, 15.14it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=12.680s: temp/gen1_chunk_000_cand_1_try1_seed176227286.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed176227286.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_050857_142_gen1_seed1058100850.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.3 seconds (0:00:00)                                          AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_050857_142_gen1_seed1058100850_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_050857_142_gen1_seed1058100850.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_050857_142_gen1_seed1058100850.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_050857_142_gen1_seed1058100850.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_050857_142_gen1_seed1058100850.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Inoffiziell? Also in Berlin ist das ziemlich offiziell, glaub mir. Ich glaube, Currywurst und DÃ¶ner kÃ¤mpfen hier tÃ¤glich um die Vorherrschaft, aber der DÃ¶ner gewinnt immer Ã¶fter. Er ist einfach... kompletter. Eine vollwertige Mahlzeit in der Hand, oder?'\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=12, content='\u001b[33mInoffiziell?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Inoffiziell?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=53, content='\u001b[33mAlso in Berlin ist das ziemlich offiziell, glaub mir....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Also in Berlin ist das ziemlich offiziell, glaub mir....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=111, content='\u001b[33mIch glaube, Currywurst und DÃ¶ner kÃ¤mpfen hier tÃ¤glich um die Vorherrschaft, aber der DÃ¶ner gewinnt immer Ã¶fter....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ich glaube, Currywurst und DÃ¶ner kÃ¤mpfen hier tÃ¤glich um die Vorherrschaft, aber der DÃ¶ner gewinnt immer Ã¶fter....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=29, content='\u001b[33mEr ist einfach... kompletter....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Er ist einfach... kompletter....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=44, content='\u001b[33mEine vollwertige Mahlzeit in der Hand, oder?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Eine vollwertige Mahlzeit in der Hand, oder?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Inoffiziell? Also in Berlin ist das ziemlich offiziell, glaub mir. Ich glaube, Currywurst und DÃ¶ner kÃ¤mpfen hier tÃ¤glich um die Vorherrschaft, aber der DÃ¶ner gewinnt immer Ã¶fter. Er ist einfach... kompletter. Eine vollwertige Mahlzeit in der Hand, oder?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=253, content='\u001b[33mInoffiziell? Also in Berlin ist das ziemlich offiziell, glaub mir. Ich glaube, Currywurst und DÃ¶ner kÃ¤mpfen hier tÃ¤glich um die Vorherrschaft, aber der DÃ¶ner gewinnt immer Ã¶fter. Er ist einfach... kompletter. Eine vollwertige Mahlzeit in der Hand, oder?...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 4118919722\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=253:\u001b[33m Inoffiziell? Also in Berlin ist das ziemlich offiziell, glaub mir. Ich glaube, Currywurst und DÃ¶ner kÃ¤mpfen hier tÃ¤glich um die Vorherrschaft, aber der DÃ¶ner gewinnt immer Ã¶fter. Er ist einfach... kompletter. Eine vollwertige Mahlzeit in der Hand, oder?\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2607320318).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  34% 339/1000 [00:23<00:42, 15.66it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  34% 340/1000 [00:23<00:45, 14.60it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=13.600s: temp/gen1_chunk_000_cand_1_try1_seed2607320318.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2607320318.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_050938_156_gen1_seed4118919722.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.35 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_050938_156_gen1_seed4118919722_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_050938_156_gen1_seed4118919722.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_050938_156_gen1_seed4118919722.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_050938_156_gen1_seed4118919722.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_050938_156_gen1_seed4118919722.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Das stimmt. Da ist alles drin. Brot, Fleisch, Salat, SoÃŸe. Alle Lebensmittelgruppen sind vertreten. Es ist quasi eine gesunde Mahlzeit... wenn man die Augen ganz fest zukneift und nicht Ã¼ber die Menge an KnoblauchsoÃŸe nachdenkt.'\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=11, content='\u001b[33mDas stimmt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das stimmt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=18, content='\u001b[33mDa ist alles drin....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Da ist alles drin....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=27, content='\u001b[33mBrot, Fleisch, Salat, SoÃŸe....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Brot, Fleisch, Salat, SoÃŸe....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=40, content='\u001b[33mAlle Lebensmittelgruppen sind vertreten....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Alle Lebensmittelgruppen sind vertreten....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=128, content='\u001b[33mEs ist quasi eine gesunde Mahlzeit... wenn man die Augen ganz fest zukneift und nicht Ã¼ber die Menge an KnoblauchsoÃŸe nachdenkt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Es ist quasi eine gesunde Mahlzeit... wenn man die Augen ganz fest zukneift und nicht Ã¼ber die Menge an KnoblauchsoÃŸe nachdenkt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Das stimmt. Da ist alles drin. Brot, Fleisch, Salat, SoÃŸe. Alle Lebensmittelgruppen sind vertreten. Es ist quasi eine gesunde Mahlzeit... wenn man die Augen ganz fest zukneift und nicht Ã¼ber die Menge an KnoblauchsoÃŸe nachdenkt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=228, content='\u001b[33mDas stimmt. Da ist alles drin. Brot, Fleisch, Salat, SoÃŸe. Alle Lebensmittelgruppen sind vertreten. Es ist quasi eine gesunde Mahlzeit... wenn man die Augen ganz fest zukneift und nicht Ã¼ber die Menge an KnoblauchsoÃŸe nachdenkt....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1271371731\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=228:\u001b[33m Das stimmt. Da ist alles drin. Brot, Fleisch, Salat, SoÃŸe. Alle Lebensmittelgruppen sind vertreten. Es ist quasi eine gesunde Mahlzeit... wenn man die Augen ganz fest zukneift und nicht Ã¼ber die Menge an KnoblauchsoÃŸe nachdenkt.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=800989753).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  36% 360/1000 [00:24<00:49, 13.02it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  36% 361/1000 [00:24<00:43, 14.62it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=14.440s: temp/gen1_chunk_000_cand_1_try1_seed800989753.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed800989753.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051022_054_gen1_seed1271371731.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.31 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051022_054_gen1_seed1271371731_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051022_054_gen1_seed1271371731.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051022_054_gen1_seed1271371731.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051022_054_gen1_seed1271371731.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051022_054_gen1_seed1271371731.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Hey, Knoblauch ist gesund! Und Salat ist Salat! Also, ich finde, wir mÃ¼ssen da heute mal eine Lanze fÃ¼r den DÃ¶ner brechen. Eine richtige Ode an den DÃ¶ner.'\n",
      "\u001b[32m[DEBUG] Split text into 4 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=26, content='\u001b[33mHey, Knoblauch ist gesund!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Hey, Knoblauch ist gesund!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=20, content='\u001b[33mUnd Salat ist Salat!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und Salat ist Salat!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=74, content='\u001b[33mAlso, ich finde, wir mÃ¼ssen da heute mal eine Lanze fÃ¼r den DÃ¶ner brechen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Also, ich finde, wir mÃ¼ssen da heute mal eine Lanze fÃ¼r den DÃ¶ner brechen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=31, content='\u001b[33mEine richtige Ode an den DÃ¶ner....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Eine richtige Ode an den DÃ¶ner....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Hey, Knoblauch ist gesund! Und Salat ist Salat! Also, ich finde, wir mÃ¼ssen da heute mal eine Lanze fÃ¼r den DÃ¶ner brechen. Eine richtige Ode an den DÃ¶ner....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=154, content='\u001b[33mHey, Knoblauch ist gesund! Und Salat ist Salat! Also, ich finde, wir mÃ¼ssen da heute mal eine Lanze fÃ¼r den DÃ¶ner brechen. Eine richtige Ode an den DÃ¶ner....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3183342587\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=154:\u001b[33m Hey, Knoblauch ist gesund! Und Salat ist Salat! Also, ich finde, wir mÃ¼ssen da heute mal eine Lanze fÃ¼r den DÃ¶ner brechen. Eine richtige Ode an den DÃ¶ner.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3981611185).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  25% 247/1000 [00:16<00:50, 14.90it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=9.880s: temp/gen1_chunk_000_cand_1_try1_seed3981611185.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3981611185.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051056_276_gen1_seed3183342587.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.2 seconds (0:00:00)                                          AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051056_276_gen1_seed3183342587_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051056_276_gen1_seed3183342587.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051056_276_gen1_seed3183342587.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051056_276_gen1_seed3183342587.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051056_276_gen1_seed3183342587.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Eine Ode an den DÃ¶ner. Boah, da hab ich Bock drauf. Okay, aber dann lass uns das mal richtig machen. Lass uns das mal komplett auseinandernehmen. Was macht den perfekten DÃ¶ner aus? Wo fangen wir an? Beim ersten DÃ¶ner unseres Lebens vielleicht?'\n",
      "\u001b[32m[DEBUG] Split text into 7 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=22, content='\u001b[33mEine Ode an den DÃ¶ner....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Eine Ode an den DÃ¶ner....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=28, content='\u001b[33mBoah, da hab ich Bock drauf....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Boah, da hab ich Bock drauf....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=48, content='\u001b[33mOkay, aber dann lass uns das mal richtig machen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Okay, aber dann lass uns das mal richtig machen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=44, content='\u001b[33mLass uns das mal komplett auseinandernehmen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Lass uns das mal komplett auseinandernehmen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=34, content='\u001b[33mWas macht den perfekten DÃ¶ner aus?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Was macht den perfekten DÃ¶ner aus?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=17, content='\u001b[33mWo fangen wir an?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Wo fangen wir an?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=44, content='\u001b[33mBeim ersten DÃ¶ner unseres Lebens vielleicht?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Beim ersten DÃ¶ner unseres Lebens vielleicht?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Eine Ode an den DÃ¶ner. Boah, da hab ich Bock drauf. Okay, aber dann lass uns das mal richtig machen. Lass uns das mal komplett auseinandernehmen. Was macht den perfekten DÃ¶ner aus? Wo fangen wir an? Beim ersten DÃ¶ner unseres Lebens vielleicht?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=243, content='\u001b[33mEine Ode an den DÃ¶ner. Boah, da hab ich Bock drauf. Okay, aber dann lass uns das mal richtig machen. Lass uns das mal komplett auseinandernehmen. Was macht den perfekten DÃ¶ner aus? Wo fangen wir an? Beim ersten DÃ¶ner unseres Lebens vielleicht?...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1639653823\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=243:\u001b[33m Eine Ode an den DÃ¶ner. Boah, da hab ich Bock drauf. Okay, aber dann lass uns das mal richtig machen. Lass uns das mal komplett auseinandernehmen. Was macht den perfekten DÃ¶ner aus? Wo fangen wir an? Beim ersten DÃ¶ner unseres Lebens vielleicht?\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3437105917).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  39% 390/1000 [00:26<00:38, 15.93it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  39% 391/1000 [00:26<00:41, 14.62it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=15.640s: temp/gen1_chunk_000_cand_1_try1_seed3437105917.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3437105917.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051141_870_gen1_seed1639653823.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.4 seconds (0:00:00)                                          AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051141_870_gen1_seed1639653823_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051141_870_gen1_seed1639653823.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051141_870_gen1_seed1639653823.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051141_870_gen1_seed1639653823.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051141_870_gen1_seed1639653823.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Oh ja! Die DÃ¶ner-Origin-Story! Das ist super. Also, pass auf... mein erster DÃ¶ner, das ist so eine ganz verschwommene, aber glÃ¼ckliche Kindheitserinnerung. Ich muss so... vielleicht acht oder neun gewesen sein. Wir waren mit meinen Eltern in irgendeiner fremden Stadt unterwegs, ich weiÃŸ nicht mal mehr wo, und es war so ein Tag, an dem alles schiefgelaufen ist.'\n",
      "\u001b[32m[DEBUG] Split text into 6 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=6, content='\u001b[33mOh ja!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Oh ja!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=23, content='\u001b[33mDie DÃ¶ner-Origin-Story!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Die DÃ¶ner-Origin-Story!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=14, content='\u001b[33mDas ist super....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist super....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=109, content='\u001b[33mAlso, pass auf... mein erster DÃ¶ner, das ist so eine ganz verschwommene, aber glÃ¼ckliche Kindheitserinnerung....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Also, pass auf... mein erster DÃ¶ner, das ist so eine ganz verschwommene, aber glÃ¼ckliche Kindheitserinnerung....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=54, content='\u001b[33mIch muss so... vielleicht acht oder neun gewesen sein....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ich muss so... vielleicht acht oder neun gewesen sein....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=151, content='\u001b[33mWir waren mit meinen Eltern in irgendeiner fremden Stadt unterwegs, ich weiÃŸ nicht mal mehr wo, und es war so ein Tag, an dem alles schiefgelaufen ist....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Oh ja! Die DÃ¶ner-Origin-Story! Das ist super. Also, pass auf... mein erster DÃ¶ner, das ist so eine ganz verschwommene, aber glÃ¼ckliche Kindheitserinnerung. Ich muss so... vielleicht acht oder neun gewesen sein....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Wir waren mit meinen Eltern in irgendeiner fremden Stadt unterwegs, ich weiÃŸ nicht mal mehr wo, und es war so ein Tag, an dem alles schiefgelaufen ist....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Wir waren mit meinen Eltern in irgendeiner fremden Stadt unterwegs, ich weiÃŸ nicht mal mehr wo, und es war so ein Tag, an dem alles schiefgelaufen ist....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=210, content='\u001b[33mOh ja! Die DÃ¶ner-Origin-Story! Das ist super. Also, pass auf... mein erster DÃ¶ner, das ist so eine ganz verschwommene, aber glÃ¼ckliche Kindheitserinnerung. Ich muss so... vielleicht acht oder neun gewesen sein....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=151, content='\u001b[33mWir waren mit meinen Eltern in irgendeiner fremden Stadt unterwegs, ich weiÃŸ nicht mal mehr wo, und es war so ein Tag, an dem alles schiefgelaufen ist....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 263855324\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=210:\u001b[33m Oh ja! Die DÃ¶ner-Origin-Story! Das ist super. Also, pass auf... mein erster DÃ¶ner, das ist so eine ganz verschwommene, aber glÃ¼ckliche Kindheitserinnerung. Ich muss so... vielleicht acht oder neun gewesen sein.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3389670804).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=151:\u001b[33m Wir waren mit meinen Eltern in irgendeiner fremden Stadt unterwegs, ich weiÃŸ nicht mal mehr wo, und es war so ein Tag, an dem alles schiefgelaufen ist.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=3389680811).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  30% 303/1000 [00:20<00:47, 14.54it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  30% 304/1000 [00:20<00:46, 14.95it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=12.160s: temp/gen1_chunk_000_cand_1_try1_seed3389670804.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  21% 208/1000 [00:14<00:49, 15.87it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ğŸš¨ Detected 2x repetition of token 6486\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:  21% 209/1000 [00:14<00:53, 14.67it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=8.360s: temp/gen1_chunk_001_cand_1_try1_seed3389680811.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3389670804.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed3389680811.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051236_992_gen1_seed263855324.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.45 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051236_992_gen1_seed263855324_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051236_992_gen1_seed263855324.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051236_992_gen1_seed263855324.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051236_992_gen1_seed263855324.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051236_992_gen1_seed263855324.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Oh oh. Was fÃ¼r ein Tag?'\n",
      "\u001b[32m[DEBUG] Split text into 2 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=6, content='\u001b[33mOh oh....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Oh oh....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=16, content='\u001b[33mWas fÃ¼r ein Tag?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Was fÃ¼r ein Tag?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Oh oh. Was fÃ¼r ein Tag?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=23, content='\u001b[33mOh oh. Was fÃ¼r ein Tag?...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3922323573\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=23:\u001b[33m Oh oh. Was fÃ¼r ein Tag?\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3701538975).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:   5% 51/1000 [00:03<01:00, 15.75it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:   5% 51/1000 [00:03<01:03, 14.98it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=2.040s: temp/gen1_chunk_000_cand_1_try1_seed3701538975.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3701538975.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051258_665_gen1_seed3922323573.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.13 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051258_665_gen1_seed3922323573_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051258_665_gen1_seed3922323573.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051258_665_gen1_seed3922323573.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051258_665_gen1_seed3922323573.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051258_665_gen1_seed3922323573.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Naja, du kennst das. Man ist im Urlaub, aber es ist kein Urlaub, sondern eher so ein anstrengender StÃ¤dtetrip mit Kindern, die keine Lust haben. Wir hatten uns verlaufen, das Wetter war schlecht, meine kleine Schwester hat nur geweint, und meine Eltern waren sichtlich genervt. Die Stimmung war komplett im Keller. Und dann standen wir vor diesem kleinen Laden, aus dem es so unglaublich gut gerochen hat. Und mein Vater, der sonst immer sehr... naja, auf deutsche Hausmannskost bedacht war, sagte plÃ¶tzlich: \"So, jetzt reicht\\'s. Wir essen jetzt DÃ¶ner.\"'\n",
      "\u001b[32m[DEBUG] Split text into 7 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=20, content='\u001b[33mNaja, du kennst das....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Naja, du kennst das....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=123, content='\u001b[33mMan ist im Urlaub, aber es ist kein Urlaub, sondern eher so ein anstrengender StÃ¤dtetrip mit Kindern, die keine Lust haben....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Man ist im Urlaub, aber es ist kein Urlaub, sondern eher so ein anstrengender StÃ¤dtetrip mit Kindern, die keine Lust haben....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=132, content='\u001b[33mWir hatten uns verlaufen, das Wetter war schlecht, meine kleine Schwester hat nur geweint, und meine Eltern waren sichtlich genervt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Wir hatten uns verlaufen, das Wetter war schlecht, meine kleine Schwester hat nur geweint, und meine Eltern waren sichtlich genervt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=36, content='\u001b[33mDie Stimmung war komplett im Keller....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Naja, du kennst das. Man ist im Urlaub, aber es ist kein Urlaub, sondern eher so ein anstrengender StÃ¤dtetrip mit Kindern, die keine Lust haben. Wir hatten uns verlaufen, das Wetter war schlecht, meine kleine Schwester hat nur geweint, und meine Eltern waren sichtlich genervt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Die Stimmung war komplett im Keller....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=90, content='\u001b[33mUnd dann standen wir vor diesem kleinen Laden, aus dem es so unglaublich gut gerochen hat....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und dann standen wir vor diesem kleinen Laden, aus dem es so unglaublich gut gerochen hat....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=123, content='\u001b[33mUnd mein Vater, der sonst immer sehr... naja, auf deutsche Hausmannskost bedacht war, sagte plÃ¶tzlich: \"So, jetzt reicht's....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und mein Vater, der sonst immer sehr... naja, auf deutsche Hausmannskost bedacht war, sagte plÃ¶tzlich: \"So, jetzt reicht's....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=23, content='\u001b[33mWir essen jetzt DÃ¶ner.\"...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Wir essen jetzt DÃ¶ner.\"...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Die Stimmung war komplett im Keller. Und dann standen wir vor diesem kleinen Laden, aus dem es so unglaublich gut gerochen hat. Und mein Vater, der sonst immer sehr... naja, auf deutsche Hausmannskost bedacht war, sagte plÃ¶tzlich: \"So, jetzt reicht's. Wir essen jetzt DÃ¶ner.\"...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=277, content='\u001b[33mNaja, du kennst das. Man ist im Urlaub, aber es ist kein Urlaub, sondern eher so ein anstrengender StÃ¤dtetrip mit Kindern, die keine Lust haben. Wir hatten uns verlaufen, das Wetter war schlecht, meine kleine Schwester hat nur geweint, und meine Eltern waren sichtlich genervt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=275, content='\u001b[33mDie Stimmung war komplett im Keller. Und dann standen wir vor diesem kleinen Laden, aus dem es so unglaublich gut gerochen hat. Und mein Vater, der sonst immer sehr... naja, auf deutsche Hausmannskost bedacht war, sagte plÃ¶tzlich: \"So, jetzt reicht's. Wir essen jetzt DÃ¶ner.\"...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 877609610\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=277:\u001b[33m Naja, du kennst das. Man ist im Urlaub, aber es ist kein Urlaub, sondern eher so ein anstrengender StÃ¤dtetrip mit Kindern, die keine Lust haben. Wir hatten uns verlaufen, das Wetter war schlecht, meine kleine Schwester hat nur geweint, und meine Eltern waren sichtlich genervt.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=100400670).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=275:\u001b[33m Die Stimmung war komplett im Keller. Und dann standen wir vor diesem kleinen Laden, aus dem es so unglaublich gut gerochen hat. Und mein Vater, der sonst immer sehr... naja, auf deutsche Hausmannskost bedacht war, sagte plÃ¶tzlich: \"So, jetzt reicht's. Wir essen jetzt DÃ¶ner.\"\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=100410677).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:   2% 17/1000 [00:01<01:02, 15.72it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ğŸš¨ Detected 2x repetition of token 6405\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:   2% 17/1000 [00:01<01:06, 14.85it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=0.680s: temp/gen1_chunk_000_cand_1_try1_seed100400670.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  36% 359/1000 [00:25<01:04,  9.94it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ğŸš¨ Detected 2x repetition of token 6486\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:  36% 359/1000 [00:25<00:45, 14.20it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=14.360s: temp/gen1_chunk_001_cand_1_try1_seed100410677.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed100400670.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed100410677.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051344_433_gen1_seed877609610.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.3 seconds (0:00:00)                                          AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051344_433_gen1_seed877609610_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051344_433_gen1_seed877609610.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051344_433_gen1_seed877609610.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051344_433_gen1_seed877609610.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051344_433_gen1_seed877609610.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Der Verzweiflungs-DÃ¶ner! Die letzte Rettung.'\n",
      "\u001b[32m[DEBUG] Split text into 2 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=24, content='\u001b[33mDer Verzweiflungs-DÃ¶ner!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Der Verzweiflungs-DÃ¶ner!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=19, content='\u001b[33mDie letzte Rettung....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Die letzte Rettung....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Der Verzweiflungs-DÃ¶ner! Die letzte Rettung....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=44, content='\u001b[33mDer Verzweiflungs-DÃ¶ner! Die letzte Rettung....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3237012497\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=44:\u001b[33m Der Verzweiflungs-DÃ¶ner! Die letzte Rettung.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=4141290099).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:   8% 84/1000 [00:06<01:07, 13.60it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=3.360s: temp/gen1_chunk_000_cand_1_try1_seed4141290099.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed4141290099.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051408_100_gen1_seed3237012497.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.13 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051408_100_gen1_seed3237012497_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051408_100_gen1_seed3237012497.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051408_100_gen1_seed3237012497.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051408_100_gen1_seed3237012497.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051408_100_gen1_seed3237012497.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Absolut! Und fÃ¼r mich als Kind war das mega aufregend. Dieses riesige Ding aus Fleisch, das sich dreht. Der Mann mit dem langen Messer, der das Fleisch so kunstvoll abgeschnitten hat. Das war wie eine Show! Und dann hat er mich gefragt: \"Mit alles?\" Und ich wusste gar nicht, was \"alles\" ist, hab aber todesmutig \"Ja!\" gesagt. Und dann hat er dieses riesige, warme Brot aufgeschnitten und all die Sachen reingepackt...'\n",
      "\u001b[32m[DEBUG] Split text into 9 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=8, content='\u001b[33mAbsolut!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Absolut!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=45, content='\u001b[33mUnd fÃ¼r mich als Kind war das mega aufregend....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und fÃ¼r mich als Kind war das mega aufregend....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=48, content='\u001b[33mDieses riesige Ding aus Fleisch, das sich dreht....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Dieses riesige Ding aus Fleisch, das sich dreht....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=79, content='\u001b[33mDer Mann mit dem langen Messer, der das Fleisch so kunstvoll abgeschnitten hat....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Der Mann mit dem langen Messer, der das Fleisch so kunstvoll abgeschnitten hat....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=22, content='\u001b[33mDas war wie eine Show!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das war wie eine Show!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=42, content='\u001b[33mUnd dann hat er mich gefragt: \"Mit alles?\"...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und dann hat er mich gefragt: \"Mit alles?\"...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=68, content='\u001b[33mUnd ich wusste gar nicht, was \"alles\" ist, hab aber todesmutig \"Ja!\"...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Absolut! Und fÃ¼r mich als Kind war das mega aufregend. Dieses riesige Ding aus Fleisch, das sich dreht. Der Mann mit dem langen Messer, der das Fleisch so kunstvoll abgeschnitten hat. Das war wie eine Show! Und dann hat er mich gefragt: \"Mit alles?\"...\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Und ich wusste gar nicht, was \"alles\" ist, hab aber todesmutig \"Ja!\"...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=7, content='\u001b[33mgesagt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: gesagt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=91, content='\u001b[33mUnd dann hat er dieses riesige, warme Brot aufgeschnitten und all die Sachen reingepackt......'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und dann hat er dieses riesige, warme Brot aufgeschnitten und all die Sachen reingepackt......\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Und ich wusste gar nicht, was \"alles\" ist, hab aber todesmutig \"Ja!\" gesagt. Und dann hat er dieses riesige, warme Brot aufgeschnitten und all die Sachen reingepackt......\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=249, content='\u001b[33mAbsolut! Und fÃ¼r mich als Kind war das mega aufregend. Dieses riesige Ding aus Fleisch, das sich dreht. Der Mann mit dem langen Messer, der das Fleisch so kunstvoll abgeschnitten hat. Das war wie eine Show! Und dann hat er mich gefragt: \"Mit alles?\"...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=168, content='\u001b[33mUnd ich wusste gar nicht, was \"alles\" ist, hab aber todesmutig \"Ja!\" gesagt. Und dann hat er dieses riesige, warme Brot aufgeschnitten und all die Sachen reingepackt......'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3656143320\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=249:\u001b[33m Absolut! Und fÃ¼r mich als Kind war das mega aufregend. Dieses riesige Ding aus Fleisch, das sich dreht. Der Mann mit dem langen Messer, der das Fleisch so kunstvoll abgeschnitten hat. Das war wie eine Show! Und dann hat er mich gefragt: \"Mit alles?\"\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=168:\u001b[33m Und ich wusste gar nicht, was \"alles\" ist, hab aber todesmutig \"Ja!\" gesagt. Und dann hat er dieses riesige, warme Brot aufgeschnitten und all die Sachen reingepackt...\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3248167816).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=3248177823).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  35% 354/1000 [00:24<00:40, 15.83it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  36% 355/1000 [00:24<00:44, 14.39it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=14.200s: temp/gen1_chunk_000_cand_1_try1_seed3248167816.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  24% 235/1000 [00:16<00:49, 15.54it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  24% 236/1000 [00:16<00:53, 14.35it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=9.440s: temp/gen1_chunk_001_cand_1_try1_seed3248177823.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3248167816.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed3248177823.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051510_639_gen1_seed3656143320.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.46 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051510_639_gen1_seed3656143320_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051510_639_gen1_seed3656143320.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051510_639_gen1_seed3656143320.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051510_639_gen1_seed3656143320.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051510_639_gen1_seed3656143320.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Boah, ich kann es mir bildlich vorstellen...'\n",
      "\u001b[32m[DEBUG] Split text into 1 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=44, content='\u001b[33mBoah, ich kann es mir bildlich vorstellen......'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Boah, ich kann es mir bildlich vorstellen......\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Boah, ich kann es mir bildlich vorstellen......\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=44, content='\u001b[33mBoah, ich kann es mir bildlich vorstellen......'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3367106247\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=44:\u001b[33m Boah, ich kann es mir bildlich vorstellen...\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3722175509).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:   8% 77/1000 [00:05<00:58, 15.85it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:   8% 77/1000 [00:05<01:11, 12.97it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=3.080s: temp/gen1_chunk_000_cand_1_try1_seed3722175509.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3722175509.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051533_517_gen1_seed3367106247.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.14 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051533_517_gen1_seed3367106247_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051533_517_gen1_seed3367106247.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051533_517_gen1_seed3367106247.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051533_517_gen1_seed3367106247.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051533_517_gen1_seed3367106247.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Und dann saÃŸen wir auf irgendeiner Parkbank, es hat leicht genieselt, und ich hab diesen ersten Bissen genommen. Und es war eine Offenbarung, Max! Dieses knusprige, aber weiche Brot, das saftige Fleisch, die frischen Tomaten und dieser Joghurt-Knoblauch-Geschmack... das hat den ganzen miesen Tag einfach ausgelÃ¶scht. Von einem Moment auf den anderen war alles wieder gut. Das war pure Magie. Ich hab die HÃ¤lfte auf meine Jacke gekleckert, aber es war mir so was von egal. Das war mein erster DÃ¶ner. Und seitdem ist er fÃ¼r mich so ein... so ein \"Alles-wird-gut-Essen\". WeiÃŸt du, was ich meine?'\n",
      "\u001b[32m[DEBUG] Split text into 9 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=112, content='\u001b[33mUnd dann saÃŸen wir auf irgendeiner Parkbank, es hat leicht genieselt, und ich hab diesen ersten Bissen genommen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und dann saÃŸen wir auf irgendeiner Parkbank, es hat leicht genieselt, und ich hab diesen ersten Bissen genommen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=33, content='\u001b[33mUnd es war eine Offenbarung, Max!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und es war eine Offenbarung, Max!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=170, content='\u001b[33mDieses knusprige, aber weiche Brot, das saftige Fleisch, die frischen Tomaten und dieser Joghurt-Knoblauch-Geschmack... das hat den ganzen miesen Tag einfach ausgelÃ¶scht....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Und dann saÃŸen wir auf irgendeiner Parkbank, es hat leicht genieselt, und ich hab diesen ersten Bissen genommen. Und es war eine Offenbarung, Max!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Dieses knusprige, aber weiche Brot, das saftige Fleisch, die frischen Tomaten und dieser Joghurt-Knoblauch-Geschmack... das hat den ganzen miesen Tag einfach ausgelÃ¶scht....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=54, content='\u001b[33mVon einem Moment auf den anderen war alles wieder gut....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Von einem Moment auf den anderen war alles wieder gut....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=19, content='\u001b[33mDas war pure Magie....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das war pure Magie....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=79, content='\u001b[33mIch hab die HÃ¤lfte auf meine Jacke gekleckert, aber es war mir so was von egal....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Dieses knusprige, aber weiche Brot, das saftige Fleisch, die frischen Tomaten und dieser Joghurt-Knoblauch-Geschmack... das hat den ganzen miesen Tag einfach ausgelÃ¶scht. Von einem Moment auf den anderen war alles wieder gut. Das war pure Magie....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Ich hab die HÃ¤lfte auf meine Jacke gekleckert, aber es war mir so was von egal....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=26, content='\u001b[33mDas war mein erster DÃ¶ner....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das war mein erster DÃ¶ner....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=68, content='\u001b[33mUnd seitdem ist er fÃ¼r mich so ein... so ein \"Alles-wird-gut-Essen\"....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und seitdem ist er fÃ¼r mich so ein... so ein \"Alles-wird-gut-Essen\"....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=24, content='\u001b[33mWeiÃŸt du, was ich meine?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: WeiÃŸt du, was ich meine?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Ich hab die HÃ¤lfte auf meine Jacke gekleckert, aber es war mir so was von egal. Das war mein erster DÃ¶ner. Und seitdem ist er fÃ¼r mich so ein... so ein \"Alles-wird-gut-Essen\". WeiÃŸt du, was ich meine?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 3\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=146, content='\u001b[33mUnd dann saÃŸen wir auf irgendeiner Parkbank, es hat leicht genieselt, und ich hab diesen ersten Bissen genommen. Und es war eine Offenbarung, Max!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=245, content='\u001b[33mDieses knusprige, aber weiche Brot, das saftige Fleisch, die frischen Tomaten und dieser Joghurt-Knoblauch-Geschmack... das hat den ganzen miesen Tag einfach ausgelÃ¶scht. Von einem Moment auf den anderen war alles wieder gut. Das war pure Magie....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 2: len=200, content='\u001b[33mIch hab die HÃ¤lfte auf meine Jacke gekleckert, aber es war mir so was von egal. Das war mein erster DÃ¶ner. Und seitdem ist er fÃ¼r mich so ein... so ein \"Alles-wird-gut-Essen\". WeiÃŸt du, was ich meine?...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1603996924\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=146:\u001b[33m Und dann saÃŸen wir auf irgendeiner Parkbank, es hat leicht genieselt, und ich hab diesen ersten Bissen genommen. Und es war eine Offenbarung, Max!\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=245:\u001b[33m Dieses knusprige, aber weiche Brot, das saftige Fleisch, die frischen Tomaten und dieser Joghurt-Knoblauch-Geschmack... das hat den ganzen miesen Tag einfach ausgelÃ¶scht. Von einem Moment auf den anderen war alles wieder gut. Das war pure Magie.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3249626612).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 2: len=200:\u001b[33m Ich hab die HÃ¤lfte auf meine Jacke gekleckert, aber es war mir so was von egal. Das war mein erster DÃ¶ner. Und seitdem ist er fÃ¼r mich so ein... so ein \"Alles-wird-gut-Essen\". WeiÃŸt du, was ich meine?\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 2 (seed=3249646626).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=3249636619).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  26% 263/1000 [00:19<00:47, 15.56it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  26% 263/1000 [00:19<00:54, 13.46it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=10.520s: temp/gen1_chunk_002_cand_1_try1_seed3249646626.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/3 (33%)\u001b[0m\n",
      "Sampling:  38% 385/1000 [00:27<00:41, 14.77it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  38% 385/1000 [00:27<00:43, 14.10it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=15.400s: temp/gen1_chunk_001_cand_1_try1_seed3249636619.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/3 (66%)\u001b[0m\n",
      "Sampling:  19% 194/1000 [00:13<00:52, 15.42it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  20% 195/1000 [00:13<00:56, 14.25it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=7.800s: temp/gen1_chunk_000_cand_1_try1_seed3249626612.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 3/3 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3249626612.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed3249636619.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_002_cand_1_try1_seed3249646626.wav as shortest candidate for chunk 2\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051657_526_gen1_seed1603996924.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.68 seconds (0:00:01)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051657_526_gen1_seed1603996924_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051657_526_gen1_seed1603996924.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051657_526_gen1_seed1603996924.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051657_526_gen1_seed1603996924.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051657_526_gen1_seed1603996924.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: \"Total. Das kann ich zu 100 Prozent nachfÃ¼hlen. Das ist nicht nur Essen, das ist ein GefÃ¼hl. Bei mir war's ein bisschen anders. Mein erster DÃ¶ner war nicht so emotional aufgeladen, sondern eher... ein Akt der Rebellion.\"\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=6, content='\u001b[33mTotal....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Total....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=39, content='\u001b[33mDas kann ich zu 100 Prozent nachfÃ¼hlen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das kann ich zu 100 Prozent nachfÃ¼hlen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=44, content='\u001b[33mDas ist nicht nur Essen, das ist ein GefÃ¼hl....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist nicht nur Essen, das ist ein GefÃ¼hl....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=34, content='\u001b[33mBei mir war's ein bisschen anders....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Bei mir war's ein bisschen anders....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=91, content='\u001b[33mMein erster DÃ¶ner war nicht so emotional aufgeladen, sondern eher... ein Akt der Rebellion....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Mein erster DÃ¶ner war nicht so emotional aufgeladen, sondern eher... ein Akt der Rebellion....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Total. Das kann ich zu 100 Prozent nachfÃ¼hlen. Das ist nicht nur Essen, das ist ein GefÃ¼hl. Bei mir war's ein bisschen anders. Mein erster DÃ¶ner war nicht so emotional aufgeladen, sondern eher... ein Akt der Rebellion....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=218, content='\u001b[33mTotal. Das kann ich zu 100 Prozent nachfÃ¼hlen. Das ist nicht nur Essen, das ist ein GefÃ¼hl. Bei mir war's ein bisschen anders. Mein erster DÃ¶ner war nicht so emotional aufgeladen, sondern eher... ein Akt der Rebellion....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 4261155103\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=218:\u001b[33m Total. Das kann ich zu 100 Prozent nachfÃ¼hlen. Das ist nicht nur Essen, das ist ein GefÃ¼hl. Bei mir war's ein bisschen anders. Mein erster DÃ¶ner war nicht so emotional aufgeladen, sondern eher... ein Akt der Rebellion.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=1983084829).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  34% 337/1000 [00:23<01:04, 10.24it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  34% 337/1000 [00:23<00:46, 14.27it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=13.480s: temp/gen1_chunk_000_cand_1_try1_seed1983084829.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed1983084829.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051742_632_gen1_seed4261155103.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.28 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051742_632_gen1_seed4261155103_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051742_632_gen1_seed4261155103.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051742_632_gen1_seed4261155103.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051742_632_gen1_seed4261155103.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051742_632_gen1_seed4261155103.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Oha, Rebellion? ErzÃ¤hl!'\n",
      "\u001b[32m[DEBUG] Split text into 2 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=15, content='\u001b[33mOha, Rebellion?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Oha, Rebellion?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=7, content='\u001b[33mErzÃ¤hl!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: ErzÃ¤hl!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Oha, Rebellion? ErzÃ¤hl!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=23, content='\u001b[33mOha, Rebellion? ErzÃ¤hl!...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1949366755\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=23:\u001b[33m Oha, Rebellion? ErzÃ¤hl!\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2911562857).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:   8% 81/1000 [00:06<00:59, 15.58it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:   8% 82/1000 [00:06<01:09, 13.18it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=3.280s: temp/gen1_chunk_000_cand_1_try1_seed2911562857.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2911562857.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051805_997_gen1_seed1949366755.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.14 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051805_997_gen1_seed1949366755_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051805_997_gen1_seed1949366755.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051805_997_gen1_seed1949366755.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051805_997_gen1_seed1949366755.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051805_997_gen1_seed1949366755.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Naja, Rebellion im kleinen Stil. Ich war vielleicht 14 oder 15 und durfte zum ersten Mal mit meinen Kumpels abends alleine weg, also so bis 10 Uhr oder so. Das war \\'ne groÃŸe Sache. Und wir hatten irgendwie unser ganzes Taschengeld zusammengekratzt. Und die Frage war: Was machen wir damit? Ins Kino? Oder in die Spielhalle? Und einer meiner Freunde meinte dann: \"Ey, lass uns zu dem DÃ¶nerladen an der Ecke gehen. Da waren wir noch nie.\"'\n",
      "\u001b[32m[DEBUG] Split text into 9 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=32, content='\u001b[33mNaja, Rebellion im kleinen Stil....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Naja, Rebellion im kleinen Stil....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=122, content='\u001b[33mIch war vielleicht 14 oder 15 und durfte zum ersten Mal mit meinen Kumpels abends alleine weg, also so bis 10 Uhr oder so....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ich war vielleicht 14 oder 15 und durfte zum ersten Mal mit meinen Kumpels abends alleine weg, also so bis 10 Uhr oder so....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=24, content='\u001b[33mDas war 'ne groÃŸe Sache....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das war 'ne groÃŸe Sache....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=67, content='\u001b[33mUnd wir hatten irgendwie unser ganzes Taschengeld zusammengekratzt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und wir hatten irgendwie unser ganzes Taschengeld zusammengekratzt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=40, content='\u001b[33mUnd die Frage war: Was machen wir damit?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und die Frage war: Was machen wir damit?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=9, content='\u001b[33mIns Kino?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ins Kino?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=23, content='\u001b[33mOder in die Spielhalle?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Naja, Rebellion im kleinen Stil. Ich war vielleicht 14 oder 15 und durfte zum ersten Mal mit meinen Kumpels abends alleine weg, also so bis 10 Uhr oder so. Das war 'ne groÃŸe Sache. Und wir hatten irgendwie unser ganzes Taschengeld zusammengekratzt. Und die Frage war: Was machen wir damit? Ins Kino?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Oder in die Spielhalle?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=88, content='\u001b[33mUnd einer meiner Freunde meinte dann: \"Ey, lass uns zu dem DÃ¶nerladen an der Ecke gehen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und einer meiner Freunde meinte dann: \"Ey, lass uns zu dem DÃ¶nerladen an der Ecke gehen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=23, content='\u001b[33mDa waren wir noch nie.\"...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Da waren wir noch nie.\"...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Oder in die Spielhalle? Und einer meiner Freunde meinte dann: \"Ey, lass uns zu dem DÃ¶nerladen an der Ecke gehen. Da waren wir noch nie.\"...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=299, content='\u001b[33mNaja, Rebellion im kleinen Stil. Ich war vielleicht 14 oder 15 und durfte zum ersten Mal mit meinen Kumpels abends alleine weg, also so bis 10 Uhr oder so. Das war 'ne groÃŸe Sache. Und wir hatten irgendwie unser ganzes Taschengeld zusammengekratzt. Und die Frage war: Was machen wir damit? Ins Kino?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=136, content='\u001b[33mOder in die Spielhalle? Und einer meiner Freunde meinte dann: \"Ey, lass uns zu dem DÃ¶nerladen an der Ecke gehen. Da waren wir noch nie.\"...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 4279233225\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=299:\u001b[33m Naja, Rebellion im kleinen Stil. Ich war vielleicht 14 oder 15 und durfte zum ersten Mal mit meinen Kumpels abends alleine weg, also so bis 10 Uhr oder so. Das war 'ne groÃŸe Sache. Und wir hatten irgendwie unser ganzes Taschengeld zusammengekratzt. Und die Frage war: Was machen wir damit? Ins Kino?\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2641970331).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=136:\u001b[33m Oder in die Spielhalle? Und einer meiner Freunde meinte dann: \"Ey, lass uns zu dem DÃ¶nerladen an der Ecke gehen. Da waren wir noch nie.\"\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=2641980338).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  44% 443/1000 [00:32<00:38, 14.32it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  44% 444/1000 [00:32<00:40, 13.84it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=17.760s: temp/gen1_chunk_000_cand_1_try1_seed2641970331.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  24% 235/1000 [00:16<00:52, 14.68it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  24% 236/1000 [00:16<00:53, 14.28it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=9.440s: temp/gen1_chunk_001_cand_1_try1_seed2641980338.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2641970331.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed2641980338.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051915_041_gen1_seed4279233225.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.57 seconds (0:00:01)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051915_041_gen1_seed4279233225_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051915_041_gen1_seed4279233225.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051915_041_gen1_seed4279233225.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051915_041_gen1_seed4279233225.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051915_041_gen1_seed4279233225.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Die verbotene Frucht.'\n",
      "\u001b[32m[DEBUG] Split text into 1 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=21, content='\u001b[33mDie verbotene Frucht....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Die verbotene Frucht....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Die verbotene Frucht....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=21, content='\u001b[33mDie verbotene Frucht....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2514732544\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=21:\u001b[33m Die verbotene Frucht.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2376651264).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  14% 140/1000 [00:09<00:55, 15.57it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  14% 140/1000 [00:09<01:00, 14.20it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=5.600s: temp/gen1_chunk_000_cand_1_try1_seed2376651264.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2376651264.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_051944_054_gen1_seed2514732544.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.16 seconds (0:00:00)                                         AM    â–ˆâ–                | 27.4%  ETA 05:19 AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_051944_054_gen1_seed2514732544_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_051944_054_gen1_seed2514732544.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_051944_054_gen1_seed2514732544.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_051944_054_gen1_seed2514732544.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_051944_054_gen1_seed2514732544.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Genau. FÃ¼r unsere Eltern war das damals noch so \"auslÃ¤ndisches Essen\", da war man skeptisch. Man wusste nicht, was da drin ist. Die GerÃ¼chte waren ja wild. Aber fÃ¼r uns war es das Coolste Ã¼berhaupt. Wir gehen also in diesen Laden, es war ein bisschen schummrig, es lief laute tÃ¼rkische Popmusik, und wir haben uns gefÃ¼hlt wie die grÃ¶ÃŸten Kings auf dem Planeten. Und wir bestellen alle \"einen DÃ¶ner mit allem und scharf\". Das war der Code. Das musste man so sagen, um dazuzugehÃ¶ren.'\n",
      "\u001b[32m[DEBUG] Split text into 9 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=6, content='\u001b[33mGenau....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Genau....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=85, content='\u001b[33mFÃ¼r unsere Eltern war das damals noch so \"auslÃ¤ndisches Essen\", da war man skeptisch....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: FÃ¼r unsere Eltern war das damals noch so \"auslÃ¤ndisches Essen\", da war man skeptisch....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=34, content='\u001b[33mMan wusste nicht, was da drin ist....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Man wusste nicht, was da drin ist....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=27, content='\u001b[33mDie GerÃ¼chte waren ja wild....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Die GerÃ¼chte waren ja wild....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=42, content='\u001b[33mAber fÃ¼r uns war es das Coolste Ã¼berhaupt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Aber fÃ¼r uns war es das Coolste Ã¼berhaupt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=162, content='\u001b[33mWir gehen also in diesen Laden, es war ein bisschen schummrig, es lief laute tÃ¼rkische Popmusik, und wir haben uns gefÃ¼hlt wie die grÃ¶ÃŸten Kings auf dem Planeten....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Genau. FÃ¼r unsere Eltern war das damals noch so \"auslÃ¤ndisches Essen\", da war man skeptisch. Man wusste nicht, was da drin ist. Die GerÃ¼chte waren ja wild. Aber fÃ¼r uns war es das Coolste Ã¼berhaupt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Wir gehen also in diesen Laden, es war ein bisschen schummrig, es lief laute tÃ¼rkische Popmusik, und wir haben uns gefÃ¼hlt wie die grÃ¶ÃŸten Kings auf dem Planeten....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=58, content='\u001b[33mUnd wir bestellen alle \"einen DÃ¶ner mit allem und scharf\"....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und wir bestellen alle \"einen DÃ¶ner mit allem und scharf\"....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=17, content='\u001b[33mDas war der Code....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das war der Code....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=42, content='\u001b[33mDas musste man so sagen, um dazuzugehÃ¶ren....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das musste man so sagen, um dazuzugehÃ¶ren....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Wir gehen also in diesen Laden, es war ein bisschen schummrig, es lief laute tÃ¼rkische Popmusik, und wir haben uns gefÃ¼hlt wie die grÃ¶ÃŸten Kings auf dem Planeten. Und wir bestellen alle \"einen DÃ¶ner mit allem und scharf\". Das war der Code. Das musste man so sagen, um dazuzugehÃ¶ren....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=198, content='\u001b[33mGenau. FÃ¼r unsere Eltern war das damals noch so \"auslÃ¤ndisches Essen\", da war man skeptisch. Man wusste nicht, was da drin ist. Die GerÃ¼chte waren ja wild. Aber fÃ¼r uns war es das Coolste Ã¼berhaupt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=282, content='\u001b[33mWir gehen also in diesen Laden, es war ein bisschen schummrig, es lief laute tÃ¼rkische Popmusik, und wir haben uns gefÃ¼hlt wie die grÃ¶ÃŸten Kings auf dem Planeten. Und wir bestellen alle \"einen DÃ¶ner mit allem und scharf\". Das war der Code. Das musste man so sagen, um dazuzugehÃ¶ren....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2626871547\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=198:\u001b[33m Genau. FÃ¼r unsere Eltern war das damals noch so \"auslÃ¤ndisches Essen\", da war man skeptisch. Man wusste nicht, was da drin ist. Die GerÃ¼chte waren ja wild. Aber fÃ¼r uns war es das Coolste Ã¼berhaupt.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=119969713).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=282:\u001b[33m Wir gehen also in diesen Laden, es war ein bisschen schummrig, es lief laute tÃ¼rkische Popmusik, und wir haben uns gefÃ¼hlt wie die grÃ¶ÃŸten Kings auf dem Planeten. Und wir bestellen alle \"einen DÃ¶ner mit allem und scharf\". Das war der Code. Das musste man so sagen, um dazuzugehÃ¶ren.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=119979720).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  31% 314/1000 [00:22<00:48, 14.11it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=12.560s: temp/gen1_chunk_000_cand_1_try1_seed119969713.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  31% 311/1000 [00:22<00:44, 15.31it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ğŸš¨ Detected 2x repetition of token 6405\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:  31% 311/1000 [00:22<00:49, 13.98it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=12.440s: temp/gen1_chunk_001_cand_1_try1_seed119979720.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed119969713.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed119979720.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052048_802_gen1_seed2626871547.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.6 seconds (0:00:01)                                          AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052048_802_gen1_seed2626871547_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052048_802_gen1_seed2626871547.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052048_802_gen1_seed2626871547.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052048_802_gen1_seed2626871547.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052048_802_gen1_seed2626871547.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: '\"Mit allem und scharf!\" Der Klassiker!'\n",
      "\u001b[32m[DEBUG] Split text into 2 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=23, content='\u001b[33m\"Mit allem und scharf!\"...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: \"Mit allem und scharf!\"...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=14, content='\u001b[33mDer Klassiker!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Der Klassiker!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: \"Mit allem und scharf!\" Der Klassiker!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=38, content='\u001b[33m\"Mit allem und scharf!\" Der Klassiker!...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 4272431213\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=38:\u001b[33m \"Mit allem und scharf!\" Der Klassiker!\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3837761159).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:   8% 76/1000 [00:04<00:58, 15.73it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=3.040s: temp/gen1_chunk_000_cand_1_try1_seed3837761159.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3837761159.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052111_159_gen1_seed4272431213.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.18 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052111_159_gen1_seed4272431213_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052111_159_gen1_seed4272431213.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052111_159_gen1_seed4272431213.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052111_159_gen1_seed4272431213.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052111_159_gen1_seed4272431213.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Und dann haben wir diese Dinger bekommen. Die waren riesig! Und \"scharf\" war wirklich scharf. Also so richtig. Mir sind die TrÃ¤nen in die Augen geschossen, meine Nase lief, aber ich hab\\'s mir natÃ¼rlich nicht anmerken lassen. Ich hab da gesessen und geschwitzt und so getan, als wÃ¤re das das Normalste der Welt. Das war so ein Initiationsritus, glaube ich. Wer den scharfen DÃ¶ner essen kann, ohne zu weinen, der ist erwachsen.'\n",
      "\u001b[32m[DEBUG] Split text into 8 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=41, content='\u001b[33mUnd dann haben wir diese Dinger bekommen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und dann haben wir diese Dinger bekommen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=17, content='\u001b[33mDie waren riesig!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Die waren riesig!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=33, content='\u001b[33mUnd \"scharf\" war wirklich scharf....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und \"scharf\" war wirklich scharf....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=16, content='\u001b[33mAlso so richtig....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Also so richtig....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=113, content='\u001b[33mMir sind die TrÃ¤nen in die Augen geschossen, meine Nase lief, aber ich hab's mir natÃ¼rlich nicht anmerken lassen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Mir sind die TrÃ¤nen in die Augen geschossen, meine Nase lief, aber ich hab's mir natÃ¼rlich nicht anmerken lassen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=85, content='\u001b[33mIch hab da gesessen und geschwitzt und so getan, als wÃ¤re das das Normalste der Welt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Und dann haben wir diese Dinger bekommen. Die waren riesig! Und \"scharf\" war wirklich scharf. Also so richtig. Mir sind die TrÃ¤nen in die Augen geschossen, meine Nase lief, aber ich hab's mir natÃ¼rlich nicht anmerken lassen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Ich hab da gesessen und geschwitzt und so getan, als wÃ¤re das das Normalste der Welt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=44, content='\u001b[33mDas war so ein Initiationsritus, glaube ich....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das war so ein Initiationsritus, glaube ich....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=69, content='\u001b[33mWer den scharfen DÃ¶ner essen kann, ohne zu weinen, der ist erwachsen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Wer den scharfen DÃ¶ner essen kann, ohne zu weinen, der ist erwachsen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Ich hab da gesessen und geschwitzt und so getan, als wÃ¤re das das Normalste der Welt. Das war so ein Initiationsritus, glaube ich. Wer den scharfen DÃ¶ner essen kann, ohne zu weinen, der ist erwachsen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=224, content='\u001b[33mUnd dann haben wir diese Dinger bekommen. Die waren riesig! Und \"scharf\" war wirklich scharf. Also so richtig. Mir sind die TrÃ¤nen in die Augen geschossen, meine Nase lief, aber ich hab's mir natÃ¼rlich nicht anmerken lassen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=200, content='\u001b[33mIch hab da gesessen und geschwitzt und so getan, als wÃ¤re das das Normalste der Welt. Das war so ein Initiationsritus, glaube ich. Wer den scharfen DÃ¶ner essen kann, ohne zu weinen, der ist erwachsen....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 17407289\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=224:\u001b[33m Und dann haben wir diese Dinger bekommen. Die waren riesig! Und \"scharf\" war wirklich scharf. Also so richtig. Mir sind die TrÃ¤nen in die Augen geschossen, meine Nase lief, aber ich hab's mir natÃ¼rlich nicht anmerken lassen.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=4133738475).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=200:\u001b[33m Ich hab da gesessen und geschwitzt und so getan, als wÃ¤re das das Normalste der Welt. Das war so ein Initiationsritus, glaube ich. Wer den scharfen DÃ¶ner essen kann, ohne zu weinen, der ist erwachsen.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=4133748482).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  34% 342/1000 [00:24<00:47, 13.95it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  34% 342/1000 [00:24<00:47, 13.85it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=13.680s: temp/gen1_chunk_001_cand_1_try1_seed4133748482.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  35% 352/1000 [00:25<00:44, 14.54it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  35% 352/1000 [00:25<00:46, 13.92it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=14.080s: temp/gen1_chunk_000_cand_1_try1_seed4133738475.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed4133738475.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed4133748482.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052221_235_gen1_seed17407289.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.55 seconds (0:00:01)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052221_235_gen1_seed17407289_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052221_235_gen1_seed17407289.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052221_235_gen1_seed17407289.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052221_235_gen1_seed17407289.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052221_235_gen1_seed17407289.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Das ist ja groÃŸartig. Ein MÃ¤nnlichkeitsritual per DÃ¶ner.'\n",
      "\u001b[32m[DEBUG] Split text into 2 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=21, content='\u001b[33mDas ist ja groÃŸartig....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist ja groÃŸartig....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=34, content='\u001b[33mEin MÃ¤nnlichkeitsritual per DÃ¶ner....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ein MÃ¤nnlichkeitsritual per DÃ¶ner....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Das ist ja groÃŸartig. Ein MÃ¤nnlichkeitsritual per DÃ¶ner....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=56, content='\u001b[33mDas ist ja groÃŸartig. Ein MÃ¤nnlichkeitsritual per DÃ¶ner....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1636641953\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=56:\u001b[33m Das ist ja groÃŸartig. Ein MÃ¤nnlichkeitsritual per DÃ¶ner.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2330144803).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  11% 113/1000 [00:08<01:04, 13.65it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  11% 114/1000 [00:08<01:04, 13.67it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=4.560s: temp/gen1_chunk_000_cand_1_try1_seed2330144803.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2330144803.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052250_222_gen1_seed1636641953.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.15 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052250_222_gen1_seed1636641953_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052250_222_gen1_seed1636641953.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052250_222_gen1_seed1636641953.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052250_222_gen1_seed1636641953.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052250_222_gen1_seed1636641953.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Quasi. Und danach haben wir uns alle unbesiegbar gefÃ¼hlt. Mit Knoblauchfahne und brennendem Mund. Das war der Geschmack der Freiheit fÃ¼r mich. Und auch wenn ich heute nicht mehr den schÃ¤rfsten DÃ¶ner bestellen muss, um mich cool zu fÃ¼hlen, ist dieses GefÃ¼hl von... UnabhÃ¤ngigkeit und ein bisschen Abenteuer immer noch damit verbunden.'\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=6, content='\u001b[33mQuasi....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Quasi....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=50, content='\u001b[33mUnd danach haben wir uns alle unbesiegbar gefÃ¼hlt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und danach haben wir uns alle unbesiegbar gefÃ¼hlt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=39, content='\u001b[33mMit Knoblauchfahne und brennendem Mund....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Mit Knoblauchfahne und brennendem Mund....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=44, content='\u001b[33mDas war der Geschmack der Freiheit fÃ¼r mich....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das war der Geschmack der Freiheit fÃ¼r mich....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=190, content='\u001b[33mUnd auch wenn ich heute nicht mehr den schÃ¤rfsten DÃ¶ner bestellen muss, um mich cool zu fÃ¼hlen, ist dieses GefÃ¼hl von... UnabhÃ¤ngigkeit und ein bisschen Abenteuer immer noch damit verbunden....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Quasi. Und danach haben wir uns alle unbesiegbar gefÃ¼hlt. Mit Knoblauchfahne und brennendem Mund. Das war der Geschmack der Freiheit fÃ¼r mich....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Und auch wenn ich heute nicht mehr den schÃ¤rfsten DÃ¶ner bestellen muss, um mich cool zu fÃ¼hlen, ist dieses GefÃ¼hl von... UnabhÃ¤ngigkeit und ein bisschen Abenteuer immer noch damit verbunden....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Und auch wenn ich heute nicht mehr den schÃ¤rfsten DÃ¶ner bestellen muss, um mich cool zu fÃ¼hlen, ist dieses GefÃ¼hl von... UnabhÃ¤ngigkeit und ein bisschen Abenteuer immer noch damit verbunden....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=142, content='\u001b[33mQuasi. Und danach haben wir uns alle unbesiegbar gefÃ¼hlt. Mit Knoblauchfahne und brennendem Mund. Das war der Geschmack der Freiheit fÃ¼r mich....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=190, content='\u001b[33mUnd auch wenn ich heute nicht mehr den schÃ¤rfsten DÃ¶ner bestellen muss, um mich cool zu fÃ¼hlen, ist dieses GefÃ¼hl von... UnabhÃ¤ngigkeit und ein bisschen Abenteuer immer noch damit verbunden....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2632916506\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=142:\u001b[33m Quasi. Und danach haben wir uns alle unbesiegbar gefÃ¼hlt. Mit Knoblauchfahne und brennendem Mund. Das war der Geschmack der Freiheit fÃ¼r mich.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2078119118).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=190:\u001b[33m Und auch wenn ich heute nicht mehr den schÃ¤rfsten DÃ¶ner bestellen muss, um mich cool zu fÃ¼hlen, ist dieses GefÃ¼hl von... UnabhÃ¤ngigkeit und ein bisschen Abenteuer immer noch damit verbunden.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=2078129125).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  23% 229/1000 [00:16<00:52, 14.59it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  23% 230/1000 [00:16<00:54, 14.21it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=9.200s: temp/gen1_chunk_000_cand_1_try1_seed2078119118.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  27% 268/1000 [00:19<00:52, 14.07it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=10.720s: temp/gen1_chunk_001_cand_1_try1_seed2078129125.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2078119118.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed2078129125.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052344_604_gen1_seed2632916506.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.44 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052344_604_gen1_seed2632916506_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052344_604_gen1_seed2632916506.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052344_604_gen1_seed2632916506.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052344_604_gen1_seed2632916506.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052344_604_gen1_seed2632916506.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Das ist es doch, oder? Jeder hat so eine Geschichte. Der DÃ¶ner ist einfach Teil des Aufwachsens in Deutschland. Das kann man gar nicht anders sagen. Aber jetzt, wo wir erwachsen und anspruchsvoll sind...'\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=22, content='\u001b[33mDas ist es doch, oder?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist es doch, oder?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=29, content='\u001b[33mJeder hat so eine Geschichte....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Jeder hat so eine Geschichte....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=58, content='\u001b[33mDer DÃ¶ner ist einfach Teil des Aufwachsens in Deutschland....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Der DÃ¶ner ist einfach Teil des Aufwachsens in Deutschland....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=36, content='\u001b[33mDas kann man gar nicht anders sagen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das kann man gar nicht anders sagen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=54, content='\u001b[33mAber jetzt, wo wir erwachsen und anspruchsvoll sind......'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Aber jetzt, wo wir erwachsen und anspruchsvoll sind......\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Das ist es doch, oder? Jeder hat so eine Geschichte. Der DÃ¶ner ist einfach Teil des Aufwachsens in Deutschland. Das kann man gar nicht anders sagen. Aber jetzt, wo wir erwachsen und anspruchsvoll sind......\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=203, content='\u001b[33mDas ist es doch, oder? Jeder hat so eine Geschichte. Der DÃ¶ner ist einfach Teil des Aufwachsens in Deutschland. Das kann man gar nicht anders sagen. Aber jetzt, wo wir erwachsen und anspruchsvoll sind......'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3486364635\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=203:\u001b[33m Das ist es doch, oder? Jeder hat so eine Geschichte. Der DÃ¶ner ist einfach Teil des Aufwachsens in Deutschland. Das kann man gar nicht anders sagen. Aber jetzt, wo wir erwachsen und anspruchsvoll sind...\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=4111042641).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  27% 268/1000 [00:19<00:50, 14.62it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  27% 268/1000 [00:19<00:53, 13.61it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=10.720s: temp/gen1_chunk_000_cand_1_try1_seed4111042641.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed4111042641.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052427_952_gen1_seed3486364635.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.31 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052427_952_gen1_seed3486364635_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052427_952_gen1_seed3486364635.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052427_952_gen1_seed3486364635.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052427_952_gen1_seed3486364635.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052427_952_gen1_seed3486364635.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Sehr anspruchsvoll.'\n",
      "\u001b[32m[DEBUG] Split text into 1 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=19, content='\u001b[33mSehr anspruchsvoll....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Sehr anspruchsvoll....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Sehr anspruchsvoll....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=19, content='\u001b[33mSehr anspruchsvoll....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1776860792\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=19:\u001b[33m Sehr anspruchsvoll.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3792488808).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:   4% 40/1000 [00:03<01:16, 12.61it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=1.600s: temp/gen1_chunk_000_cand_1_try1_seed3792488808.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3792488808.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052447_237_gen1_seed1776860792.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.14 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052447_237_gen1_seed1776860792_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052447_237_gen1_seed1776860792.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052447_237_gen1_seed1776860792.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052447_237_gen1_seed1776860792.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052447_237_gen1_seed1776860792.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: '...mÃ¼ssen wir mal Ã¼ber die QualitÃ¤t sprechen. Warte, ich muss da mal kurz ausholen. Es gibt ja so viele DÃ¶nerbuden. Allein bei mir im Kiez in Berlin gibt es, glaube ich, sechs StÃ¼ck in einem Radius von 500 Metern. Und die sind nicht alle gleich gut. Da gibt es riesige Unterschiede. Das ist eine Wissenschaft fÃ¼r sich.'\n",
      "\u001b[32m[DEBUG] Split text into 7 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=45, content='\u001b[33m...mÃ¼ssen wir mal Ã¼ber die QualitÃ¤t sprechen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: ...mÃ¼ssen wir mal Ã¼ber die QualitÃ¤t sprechen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=37, content='\u001b[33mWarte, ich muss da mal kurz ausholen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Warte, ich muss da mal kurz ausholen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=31, content='\u001b[33mEs gibt ja so viele DÃ¶nerbuden....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Es gibt ja so viele DÃ¶nerbuden....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=97, content='\u001b[33mAllein bei mir im Kiez in Berlin gibt es, glaube ich, sechs StÃ¼ck in einem Radius von 500 Metern....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Allein bei mir im Kiez in Berlin gibt es, glaube ich, sechs StÃ¼ck in einem Radius von 500 Metern....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=35, content='\u001b[33mUnd die sind nicht alle gleich gut....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und die sind nicht alle gleich gut....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=32, content='\u001b[33mDa gibt es riesige Unterschiede....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Da gibt es riesige Unterschiede....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=35, content='\u001b[33mDas ist eine Wissenschaft fÃ¼r sich....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: ...mÃ¼ssen wir mal Ã¼ber die QualitÃ¤t sprechen. Warte, ich muss da mal kurz ausholen. Es gibt ja so viele DÃ¶nerbuden. Allein bei mir im Kiez in Berlin gibt es, glaube ich, sechs StÃ¼ck in einem Radius von 500 Metern. Und die sind nicht alle gleich gut. Da gibt es riesige Unterschiede....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Das ist eine Wissenschaft fÃ¼r sich....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Das ist eine Wissenschaft fÃ¼r sich....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=282, content='\u001b[33m...mÃ¼ssen wir mal Ã¼ber die QualitÃ¤t sprechen. Warte, ich muss da mal kurz ausholen. Es gibt ja so viele DÃ¶nerbuden. Allein bei mir im Kiez in Berlin gibt es, glaube ich, sechs StÃ¼ck in einem Radius von 500 Metern. Und die sind nicht alle gleich gut. Da gibt es riesige Unterschiede....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=35, content='\u001b[33mDas ist eine Wissenschaft fÃ¼r sich....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 353023846\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=282:\u001b[33m ...mÃ¼ssen wir mal Ã¼ber die QualitÃ¤t sprechen. Warte, ich muss da mal kurz ausholen. Es gibt ja so viele DÃ¶nerbuden. Allein bei mir im Kiez in Berlin gibt es, glaube ich, sechs StÃ¼ck in einem Radius von 500 Metern. Und die sind nicht alle gleich gut. Da gibt es riesige Unterschiede.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=68176818).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=35:\u001b[33m Das ist eine Wissenschaft fÃ¼r sich.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=68186825).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  38% 385/1000 [00:28<00:43, 14.16it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  39% 386/1000 [00:28<00:44, 13.65it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=15.440s: temp/gen1_chunk_000_cand_1_try1_seed68176818.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:   7% 72/1000 [00:05<01:02, 14.77it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:   7% 72/1000 [00:05<01:13, 12.62it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=2.880s: temp/gen1_chunk_001_cand_1_try1_seed68186825.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed68176818.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed68186825.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052538_992_gen1_seed353023846.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.42 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052538_992_gen1_seed353023846_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052538_992_gen1_seed353023846.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052538_992_gen1_seed353023846.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052538_992_gen1_seed353023846.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052538_992_gen1_seed353023846.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Okay, warte, warte. Stopp. Lass uns das mal systematisch angehen. Ich brauche Struktur, Anna, sonst verliere ich den Ãœberblick bei deiner enthusiastischen Art. Was sind die entscheidenden Komponenten fÃ¼r einen DÃ¶ner, der von \"okay\" zu \"legendÃ¤r\" wird? Ich sage, es gibt vier SÃ¤ulen.'\n",
      "\u001b[32m[DEBUG] Split text into 6 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=19, content='\u001b[33mOkay, warte, warte....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Okay, warte, warte....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=6, content='\u001b[33mStopp....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Stopp....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=38, content='\u001b[33mLass uns das mal systematisch angehen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Lass uns das mal systematisch angehen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=93, content='\u001b[33mIch brauche Struktur, Anna, sonst verliere ich den Ãœberblick bei deiner enthusiastischen Art....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ich brauche Struktur, Anna, sonst verliere ich den Ãœberblick bei deiner enthusiastischen Art....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=91, content='\u001b[33mWas sind die entscheidenden Komponenten fÃ¼r einen DÃ¶ner, der von \"okay\" zu \"legendÃ¤r\" wird?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Was sind die entscheidenden Komponenten fÃ¼r einen DÃ¶ner, der von \"okay\" zu \"legendÃ¤r\" wird?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=30, content='\u001b[33mIch sage, es gibt vier SÃ¤ulen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ich sage, es gibt vier SÃ¤ulen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Okay, warte, warte. Stopp. Lass uns das mal systematisch angehen. Ich brauche Struktur, Anna, sonst verliere ich den Ãœberblick bei deiner enthusiastischen Art. Was sind die entscheidenden Komponenten fÃ¼r einen DÃ¶ner, der von \"okay\" zu \"legendÃ¤r\" wird? Ich sage, es gibt vier SÃ¤ulen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=282, content='\u001b[33mOkay, warte, warte. Stopp. Lass uns das mal systematisch angehen. Ich brauche Struktur, Anna, sonst verliere ich den Ãœberblick bei deiner enthusiastischen Art. Was sind die entscheidenden Komponenten fÃ¼r einen DÃ¶ner, der von \"okay\" zu \"legendÃ¤r\" wird? Ich sage, es gibt vier SÃ¤ulen....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1244398177\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=282:\u001b[33m Okay, warte, warte. Stopp. Lass uns das mal systematisch angehen. Ich brauche Struktur, Anna, sonst verliere ich den Ãœberblick bei deiner enthusiastischen Art. Was sind die entscheidenden Komponenten fÃ¼r einen DÃ¶ner, der von \"okay\" zu \"legendÃ¤r\" wird? Ich sage, es gibt vier SÃ¤ulen.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3855655267).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  41% 413/1000 [00:30<00:41, 14.24it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  41% 413/1000 [00:30<00:43, 13.38it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=16.520s: temp/gen1_chunk_000_cand_1_try1_seed3855655267.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3855655267.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052630_923_gen1_seed1244398177.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.37 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052630_923_gen1_seed1244398177_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052630_923_gen1_seed1244398177.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052630_923_gen1_seed1244398177.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052630_923_gen1_seed1244398177.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052630_923_gen1_seed1244398177.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Vier SÃ¤ulen? Okay, schieÃŸ los, Herr Analytiker.'\n",
      "\u001b[32m[DEBUG] Split text into 2 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=12, content='\u001b[33mVier SÃ¤ulen?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Vier SÃ¤ulen?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=34, content='\u001b[33mOkay, schieÃŸ los, Herr Analytiker....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Okay, schieÃŸ los, Herr Analytiker....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Vier SÃ¤ulen? Okay, schieÃŸ los, Herr Analytiker....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=47, content='\u001b[33mVier SÃ¤ulen? Okay, schieÃŸ los, Herr Analytiker....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2751423604\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=47:\u001b[33m Vier SÃ¤ulen? Okay, schieÃŸ los, Herr Analytiker.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2794009180).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:   9% 94/1000 [00:06<01:01, 14.70it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  10% 95/1000 [00:06<01:00, 14.95it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=3.800s: temp/gen1_chunk_000_cand_1_try1_seed2794009180.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2794009180.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052656_362_gen1_seed2751423604.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.19 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052656_362_gen1_seed2751423604_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052656_362_gen1_seed2751423604.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052656_362_gen1_seed2751423604.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052656_362_gen1_seed2751423604.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052656_362_gen1_seed2751423604.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'SÃ¤ule eins: Das Brot. SÃ¤ule zwei: Das Fleisch. SÃ¤ule drei: Der Salat. Und die alles entscheidende, die alles zusammenhaltende vierte SÃ¤ule: Die SoÃŸen. Stimmst du mir da zu?'\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=21, content='\u001b[33mSÃ¤ule eins: Das Brot....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: SÃ¤ule eins: Das Brot....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=24, content='\u001b[33mSÃ¤ule zwei: Das Fleisch....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: SÃ¤ule zwei: Das Fleisch....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=22, content='\u001b[33mSÃ¤ule drei: Der Salat....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: SÃ¤ule drei: Der Salat....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=80, content='\u001b[33mUnd die alles entscheidende, die alles zusammenhaltende vierte SÃ¤ule: Die SoÃŸen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und die alles entscheidende, die alles zusammenhaltende vierte SÃ¤ule: Die SoÃŸen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=21, content='\u001b[33mStimmst du mir da zu?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Stimmst du mir da zu?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: SÃ¤ule eins: Das Brot. SÃ¤ule zwei: Das Fleisch. SÃ¤ule drei: Der Salat. Und die alles entscheidende, die alles zusammenhaltende vierte SÃ¤ule: Die SoÃŸen. Stimmst du mir da zu?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=172, content='\u001b[33mSÃ¤ule eins: Das Brot. SÃ¤ule zwei: Das Fleisch. SÃ¤ule drei: Der Salat. Und die alles entscheidende, die alles zusammenhaltende vierte SÃ¤ule: Die SoÃŸen. Stimmst du mir da zu?...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3667964733\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=172:\u001b[33m SÃ¤ule eins: Das Brot. SÃ¤ule zwei: Das Fleisch. SÃ¤ule drei: Der Salat. Und die alles entscheidende, die alles zusammenhaltende vierte SÃ¤ule: Die SoÃŸen. Stimmst du mir da zu?\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=651666167).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  29% 286/1000 [00:20<00:48, 14.86it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  29% 286/1000 [00:20<00:50, 14.03it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=11.440s: temp/gen1_chunk_000_cand_1_try1_seed651666167.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed651666167.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052733_432_gen1_seed3667964733.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.3 seconds (0:00:00)                                          AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052733_432_gen1_seed3667964733_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052733_432_gen1_seed3667964733.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052733_432_gen1_seed3667964733.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052733_432_gen1_seed3667964733.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052733_432_gen1_seed3667964733.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Ja, das ist eine gute Struktur. Die vier heiligen SÃ¤ulen des DÃ¶ner-Tempels. Okay, fangen wir mit dem Brot an. FÃ¼r mich das A und O. Ehrlich. Ein guter DÃ¶ner kann durch schlechtes Brot komplett ruiniert werden.'\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=31, content='\u001b[33mJa, das ist eine gute Struktur....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ja, das ist eine gute Struktur....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=43, content='\u001b[33mDie vier heiligen SÃ¤ulen des DÃ¶ner-Tempels....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Die vier heiligen SÃ¤ulen des DÃ¶ner-Tempels....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=33, content='\u001b[33mOkay, fangen wir mit dem Brot an....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Okay, fangen wir mit dem Brot an....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=30, content='\u001b[33mFÃ¼r mich das A und O. Ehrlich....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: FÃ¼r mich das A und O. Ehrlich....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=68, content='\u001b[33mEin guter DÃ¶ner kann durch schlechtes Brot komplett ruiniert werden....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ein guter DÃ¶ner kann durch schlechtes Brot komplett ruiniert werden....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Ja, das ist eine gute Struktur. Die vier heiligen SÃ¤ulen des DÃ¶ner-Tempels. Okay, fangen wir mit dem Brot an. FÃ¼r mich das A und O. Ehrlich. Ein guter DÃ¶ner kann durch schlechtes Brot komplett ruiniert werden....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=209, content='\u001b[33mJa, das ist eine gute Struktur. Die vier heiligen SÃ¤ulen des DÃ¶ner-Tempels. Okay, fangen wir mit dem Brot an. FÃ¼r mich das A und O. Ehrlich. Ein guter DÃ¶ner kann durch schlechtes Brot komplett ruiniert werden....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1765149051\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=209:\u001b[33m Ja, das ist eine gute Struktur. Die vier heiligen SÃ¤ulen des DÃ¶ner-Tempels. Okay, fangen wir mit dem Brot an. FÃ¼r mich das A und O. Ehrlich. Ein guter DÃ¶ner kann durch schlechtes Brot komplett ruiniert werden.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=97202481).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  30% 295/1000 [00:21<00:48, 14.56it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  30% 295/1000 [00:21<00:52, 13.48it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=11.800s: temp/gen1_chunk_000_cand_1_try1_seed97202481.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed97202481.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052813_552_gen1_seed1765149051.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.32 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052813_552_gen1_seed1765149051_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052813_552_gen1_seed1765149051.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052813_552_gen1_seed1765149051.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052813_552_gen1_seed1765149051.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052813_552_gen1_seed1765149051.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Absolut! Nichts ist schlimmer als so ein labberiges, kaltes Fladenbrot, das schon beim Angucken durchweicht. Oder, noch schlimmer, so ein aufgebackenes Billig-Teil aus der PlastiktÃ¼te, das irgendwie nach Pappe schmeckt. Boah, da krieg ich die Krise.'\n",
      "\u001b[32m[DEBUG] Split text into 4 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=8, content='\u001b[33mAbsolut!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Absolut!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=99, content='\u001b[33mNichts ist schlimmer als so ein labberiges, kaltes Fladenbrot, das schon beim Angucken durchweicht....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Nichts ist schlimmer als so ein labberiges, kaltes Fladenbrot, das schon beim Angucken durchweicht....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=110, content='\u001b[33mOder, noch schlimmer, so ein aufgebackenes Billig-Teil aus der PlastiktÃ¼te, das irgendwie nach Pappe schmeckt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Oder, noch schlimmer, so ein aufgebackenes Billig-Teil aus der PlastiktÃ¼te, das irgendwie nach Pappe schmeckt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=29, content='\u001b[33mBoah, da krieg ich die Krise....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Boah, da krieg ich die Krise....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Absolut! Nichts ist schlimmer als so ein labberiges, kaltes Fladenbrot, das schon beim Angucken durchweicht. Oder, noch schlimmer, so ein aufgebackenes Billig-Teil aus der PlastiktÃ¼te, das irgendwie nach Pappe schmeckt. Boah, da krieg ich die Krise....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=249, content='\u001b[33mAbsolut! Nichts ist schlimmer als so ein labberiges, kaltes Fladenbrot, das schon beim Angucken durchweicht. Oder, noch schlimmer, so ein aufgebackenes Billig-Teil aus der PlastiktÃ¼te, das irgendwie nach Pappe schmeckt. Boah, da krieg ich die Krise....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1664790342\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=249:\u001b[33m Absolut! Nichts ist schlimmer als so ein labberiges, kaltes Fladenbrot, das schon beim Angucken durchweicht. Oder, noch schlimmer, so ein aufgebackenes Billig-Teil aus der PlastiktÃ¼te, das irgendwie nach Pappe schmeckt. Boah, da krieg ich die Krise.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=1587931986).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  35% 354/1000 [00:25<00:44, 14.63it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  36% 355/1000 [00:26<00:47, 13.63it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=14.200s: temp/gen1_chunk_000_cand_1_try1_seed1587931986.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed1587931986.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_052906_991_gen1_seed1664790342.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.32 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_052906_991_gen1_seed1664790342_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_052906_991_gen1_seed1664790342.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_052906_991_gen1_seed1664790342.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_052906_991_gen1_seed1664790342.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_052906_991_gen1_seed1664790342.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Ja, furchtbar! Das Brot muss auÃŸen knusprig sein. Es muss dieses GerÃ¤usch machen, wenn man reinbeiÃŸt. So ein leichtes Knacken. Und innen muss es weich und fluffig sein, aber trotzdem stabil genug, um die ganze FÃ¼llung zu halten. Viele gute DÃ¶nerlÃ¤den backen ihr Brot ja selbst. Das ist immer ein gutes Zeichen. Wenn du reinkommst und es riecht nicht nur nach Fleisch, sondern auch nach frischem Brot, dann bist du richtig.'\n",
      "\u001b[32m[DEBUG] Split text into 8 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=14, content='\u001b[33mJa, furchtbar!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ja, furchtbar!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=34, content='\u001b[33mDas Brot muss auÃŸen knusprig sein....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das Brot muss auÃŸen knusprig sein....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=51, content='\u001b[33mEs muss dieses GerÃ¤usch machen, wenn man reinbeiÃŸt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Es muss dieses GerÃ¤usch machen, wenn man reinbeiÃŸt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=24, content='\u001b[33mSo ein leichtes Knacken....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: So ein leichtes Knacken....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=101, content='\u001b[33mUnd innen muss es weich und fluffig sein, aber trotzdem stabil genug, um die ganze FÃ¼llung zu halten....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und innen muss es weich und fluffig sein, aber trotzdem stabil genug, um die ganze FÃ¼llung zu halten....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=48, content='\u001b[33mViele gute DÃ¶nerlÃ¤den backen ihr Brot ja selbst....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Viele gute DÃ¶nerlÃ¤den backen ihr Brot ja selbst....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=32, content='\u001b[33mDas ist immer ein gutes Zeichen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Ja, furchtbar! Das Brot muss auÃŸen knusprig sein. Es muss dieses GerÃ¤usch machen, wenn man reinbeiÃŸt. So ein leichtes Knacken. Und innen muss es weich und fluffig sein, aber trotzdem stabil genug, um die ganze FÃ¼llung zu halten. Viele gute DÃ¶nerlÃ¤den backen ihr Brot ja selbst....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Das ist immer ein gutes Zeichen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=111, content='\u001b[33mWenn du reinkommst und es riecht nicht nur nach Fleisch, sondern auch nach frischem Brot, dann bist du richtig....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Wenn du reinkommst und es riecht nicht nur nach Fleisch, sondern auch nach frischem Brot, dann bist du richtig....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Das ist immer ein gutes Zeichen. Wenn du reinkommst und es riecht nicht nur nach Fleisch, sondern auch nach frischem Brot, dann bist du richtig....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=277, content='\u001b[33mJa, furchtbar! Das Brot muss auÃŸen knusprig sein. Es muss dieses GerÃ¤usch machen, wenn man reinbeiÃŸt. So ein leichtes Knacken. Und innen muss es weich und fluffig sein, aber trotzdem stabil genug, um die ganze FÃ¼llung zu halten. Viele gute DÃ¶nerlÃ¤den backen ihr Brot ja selbst....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=144, content='\u001b[33mDas ist immer ein gutes Zeichen. Wenn du reinkommst und es riecht nicht nur nach Fleisch, sondern auch nach frischem Brot, dann bist du richtig....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2467636839\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=277:\u001b[33m Ja, furchtbar! Das Brot muss auÃŸen knusprig sein. Es muss dieses GerÃ¤usch machen, wenn man reinbeiÃŸt. So ein leichtes Knacken. Und innen muss es weich und fluffig sein, aber trotzdem stabil genug, um die ganze FÃ¼llung zu halten. Viele gute DÃ¶nerlÃ¤den backen ihr Brot ja selbst.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=846764789).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=144:\u001b[33m Das ist immer ein gutes Zeichen. Wenn du reinkommst und es riecht nicht nur nach Fleisch, sondern auch nach frischem Brot, dann bist du richtig.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=846774796).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  39% 391/1000 [00:28<00:42, 14.17it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  39% 391/1000 [00:28<00:44, 13.58it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=15.640s: temp/gen1_chunk_000_cand_1_try1_seed846764789.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  20% 200/1000 [00:15<00:56, 14.05it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  20% 200/1000 [00:15<01:00, 13.24it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=8.000s: temp/gen1_chunk_001_cand_1_try1_seed846774796.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed846764789.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed846774796.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_053021_507_gen1_seed2467636839.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.49 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_053021_507_gen1_seed2467636839_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_053021_507_gen1_seed2467636839.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_053021_507_gen1_seed2467636839.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_053021_507_gen1_seed2467636839.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_053021_507_gen1_seed2467636839.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Und es gibt ja verschiedene Arten. Das klassische Fladenbrot, also dieses runde, das dann zu einer Tasche geschnitten wird. Aber es gibt ja auch DÃ¼rÃ¼m, also den gerollten DÃ¶ner in diesem dÃ¼nnen Yufka-Teig. Bist du da Team Tasche oder Team Rolle?'\n",
      "\u001b[32m[DEBUG] Split text into 4 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=34, content='\u001b[33mUnd es gibt ja verschiedene Arten....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und es gibt ja verschiedene Arten....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=88, content='\u001b[33mDas klassische Fladenbrot, also dieses runde, das dann zu einer Tasche geschnitten wird....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das klassische Fladenbrot, also dieses runde, das dann zu einer Tasche geschnitten wird....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=81, content='\u001b[33mAber es gibt ja auch DÃ¼rÃ¼m, also den gerollten DÃ¶ner in diesem dÃ¼nnen Yufka-Teig....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Aber es gibt ja auch DÃ¼rÃ¼m, also den gerollten DÃ¶ner in diesem dÃ¼nnen Yufka-Teig....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=39, content='\u001b[33mBist du da Team Tasche oder Team Rolle?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Bist du da Team Tasche oder Team Rolle?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Und es gibt ja verschiedene Arten. Das klassische Fladenbrot, also dieses runde, das dann zu einer Tasche geschnitten wird. Aber es gibt ja auch DÃ¼rÃ¼m, also den gerollten DÃ¶ner in diesem dÃ¼nnen Yufka-Teig. Bist du da Team Tasche oder Team Rolle?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=245, content='\u001b[33mUnd es gibt ja verschiedene Arten. Das klassische Fladenbrot, also dieses runde, das dann zu einer Tasche geschnitten wird. Aber es gibt ja auch DÃ¼rÃ¼m, also den gerollten DÃ¶ner in diesem dÃ¼nnen Yufka-Teig. Bist du da Team Tasche oder Team Rolle?...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1600525065\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=245:\u001b[33m Und es gibt ja verschiedene Arten. Das klassische Fladenbrot, also dieses runde, das dann zu einer Tasche geschnitten wird. Aber es gibt ja auch DÃ¼rÃ¼m, also den gerollten DÃ¶ner in diesem dÃ¼nnen Yufka-Teig. Bist du da Team Tasche oder Team Rolle?\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=1713786203).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  36% 361/1000 [00:26<00:44, 14.40it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  36% 362/1000 [00:26<00:46, 13.71it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=14.480s: temp/gen1_chunk_000_cand_1_try1_seed1713786203.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed1713786203.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_053116_613_gen1_seed1600525065.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.38 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_053116_613_gen1_seed1600525065_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_053116_613_gen1_seed1600525065.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_053116_613_gen1_seed1600525065.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_053116_613_gen1_seed1600525065.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_053116_613_gen1_seed1600525065.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Puh, schwierige Frage. Das ist fast eine philosophische Frage. Also, ich wÃ¼rde sagen, zu 90 Prozent bin ich Team Tasche. Weil ich dieses VerhÃ¤ltnis von knusprig zu weich beim Fladenbrot so liebe. Der DÃ¼rÃ¼m hat aber auch seine Vorteile. Er ist... transportabler. Man kleckert nicht so leicht. Der DÃ¼rÃ¼m ist der DÃ¶ner fÃ¼r unterwegs, wenn man eine weiÃŸe Hose anhat.'\n",
      "\u001b[32m[DEBUG] Split text into 8 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=22, content='\u001b[33mPuh, schwierige Frage....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Puh, schwierige Frage....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=39, content='\u001b[33mDas ist fast eine philosophische Frage....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist fast eine philosophische Frage....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=57, content='\u001b[33mAlso, ich wÃ¼rde sagen, zu 90 Prozent bin ich Team Tasche....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Also, ich wÃ¼rde sagen, zu 90 Prozent bin ich Team Tasche....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=74, content='\u001b[33mWeil ich dieses VerhÃ¤ltnis von knusprig zu weich beim Fladenbrot so liebe....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Weil ich dieses VerhÃ¤ltnis von knusprig zu weich beim Fladenbrot so liebe....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=39, content='\u001b[33mDer DÃ¼rÃ¼m hat aber auch seine Vorteile....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Der DÃ¼rÃ¼m hat aber auch seine Vorteile....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=25, content='\u001b[33mEr ist... transportabler....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Er ist... transportabler....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=29, content='\u001b[33mMan kleckert nicht so leicht....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Man kleckert nicht so leicht....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=70, content='\u001b[33mDer DÃ¼rÃ¼m ist der DÃ¶ner fÃ¼r unterwegs, wenn man eine weiÃŸe Hose anhat....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Puh, schwierige Frage. Das ist fast eine philosophische Frage. Also, ich wÃ¼rde sagen, zu 90 Prozent bin ich Team Tasche. Weil ich dieses VerhÃ¤ltnis von knusprig zu weich beim Fladenbrot so liebe. Der DÃ¼rÃ¼m hat aber auch seine Vorteile. Er ist... transportabler. Man kleckert nicht so leicht....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Der DÃ¼rÃ¼m ist der DÃ¶ner fÃ¼r unterwegs, wenn man eine weiÃŸe Hose anhat....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Der DÃ¼rÃ¼m ist der DÃ¶ner fÃ¼r unterwegs, wenn man eine weiÃŸe Hose anhat....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=291, content='\u001b[33mPuh, schwierige Frage. Das ist fast eine philosophische Frage. Also, ich wÃ¼rde sagen, zu 90 Prozent bin ich Team Tasche. Weil ich dieses VerhÃ¤ltnis von knusprig zu weich beim Fladenbrot so liebe. Der DÃ¼rÃ¼m hat aber auch seine Vorteile. Er ist... transportabler. Man kleckert nicht so leicht....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=70, content='\u001b[33mDer DÃ¼rÃ¼m ist der DÃ¶ner fÃ¼r unterwegs, wenn man eine weiÃŸe Hose anhat....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3943767190\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=291:\u001b[33m Puh, schwierige Frage. Das ist fast eine philosophische Frage. Also, ich wÃ¼rde sagen, zu 90 Prozent bin ich Team Tasche. Weil ich dieses VerhÃ¤ltnis von knusprig zu weich beim Fladenbrot so liebe. Der DÃ¼rÃ¼m hat aber auch seine Vorteile. Er ist... transportabler. Man kleckert nicht so leicht.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2611160898).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=70:\u001b[33m Der DÃ¼rÃ¼m ist der DÃ¶ner fÃ¼r unterwegs, wenn man eine weiÃŸe Hose anhat.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=2611170905).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  43% 427/1000 [00:32<00:41, 13.95it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  43% 427/1000 [00:32<00:43, 13.09it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=17.080s: temp/gen1_chunk_000_cand_1_try1_seed2611160898.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  42% 415/1000 [00:31<00:41, 14.06it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ğŸš¨ Detected 2x repetition of token 6486\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:  42% 415/1000 [00:31<00:43, 13.35it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=16.600s: temp/gen1_chunk_001_cand_1_try1_seed2611170905.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2611160898.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed2611170905.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_053248_130_gen1_seed3943767190.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.7 seconds (0:00:01)                                          AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_053248_130_gen1_seed3943767190_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_053248_130_gen1_seed3943767190.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_053248_130_gen1_seed3943767190.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_053248_130_gen1_seed3943767190.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_053248_130_gen1_seed3943767190.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Der \"weiÃŸe-Hose-DÃ¶ner\"! Perfekte Kategorie. Das stimmt. Beim DÃ¶ner in der Tasche gibt es ja immer diesen kritischen Punkt, meistens so im letzten Drittel, wo die strukturelle IntegritÃ¤t des Brotes versagt und die SoÃŸe unten raustropft. Das ist ein Moment von hÃ¶chster Anspannung.'\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=23, content='\u001b[33mDer \"weiÃŸe-Hose-DÃ¶ner\"!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Der \"weiÃŸe-Hose-DÃ¶ner\"!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=19, content='\u001b[33mPerfekte Kategorie....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Perfekte Kategorie....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=11, content='\u001b[33mDas stimmt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das stimmt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=179, content='\u001b[33mBeim DÃ¶ner in der Tasche gibt es ja immer diesen kritischen Punkt, meistens so im letzten Drittel, wo die strukturelle IntegritÃ¤t des Brotes versagt und die SoÃŸe unten raustropft....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Beim DÃ¶ner in der Tasche gibt es ja immer diesen kritischen Punkt, meistens so im letzten Drittel, wo die strukturelle IntegritÃ¤t des Brotes versagt und die SoÃŸe unten raustropft....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=43, content='\u001b[33mDas ist ein Moment von hÃ¶chster Anspannung....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist ein Moment von hÃ¶chster Anspannung....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Der \"weiÃŸe-Hose-DÃ¶ner\"! Perfekte Kategorie. Das stimmt. Beim DÃ¶ner in der Tasche gibt es ja immer diesen kritischen Punkt, meistens so im letzten Drittel, wo die strukturelle IntegritÃ¤t des Brotes versagt und die SoÃŸe unten raustropft. Das ist ein Moment von hÃ¶chster Anspannung....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=279, content='\u001b[33mDer \"weiÃŸe-Hose-DÃ¶ner\"! Perfekte Kategorie. Das stimmt. Beim DÃ¶ner in der Tasche gibt es ja immer diesen kritischen Punkt, meistens so im letzten Drittel, wo die strukturelle IntegritÃ¤t des Brotes versagt und die SoÃŸe unten raustropft. Das ist ein Moment von hÃ¶chster Anspannung....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3687135472\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=279:\u001b[33m Der \"weiÃŸe-Hose-DÃ¶ner\"! Perfekte Kategorie. Das stimmt. Beim DÃ¶ner in der Tasche gibt es ja immer diesen kritischen Punkt, meistens so im letzten Drittel, wo die strukturelle IntegritÃ¤t des Brotes versagt und die SoÃŸe unten raustropft. Das ist ein Moment von hÃ¶chster Anspannung.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3009136336).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  43% 433/1000 [00:32<00:43, 13.12it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  43% 433/1000 [00:32<00:42, 13.31it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=17.320s: temp/gen1_chunk_000_cand_1_try1_seed3009136336.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3009136336.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_053344_490_gen1_seed3687135472.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.35 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_053344_490_gen1_seed3687135472_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_053344_490_gen1_seed3687135472.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_053344_490_gen1_seed3687135472.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_053344_490_gen1_seed3687135472.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_053344_490_gen1_seed3687135472.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Oh ja! Der Moment, in dem du den DÃ¶ner so komisch neigen und von unten hochschieben musst, wÃ¤hrend du versuchst, die heraustropfende SoÃŸe mit der Papierserviette aufzufangen. Das ist Akrobatik. Manchmal beiÃŸe ich auch extra unten eine kleine Ecke ab und sauge die SoÃŸe raus, bevor die Katastrophe passiert. Ist das komisch?'\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=6, content='\u001b[33mOh ja!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Oh ja!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=167, content='\u001b[33mDer Moment, in dem du den DÃ¶ner so komisch neigen und von unten hochschieben musst, wÃ¤hrend du versuchst, die heraustropfende SoÃŸe mit der Papierserviette aufzufangen....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Der Moment, in dem du den DÃ¶ner so komisch neigen und von unten hochschieben musst, wÃ¤hrend du versuchst, die heraustropfende SoÃŸe mit der Papierserviette aufzufangen....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=18, content='\u001b[33mDas ist Akrobatik....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist Akrobatik....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=112, content='\u001b[33mManchmal beiÃŸe ich auch extra unten eine kleine Ecke ab und sauge die SoÃŸe raus, bevor die Katastrophe passiert....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Oh ja! Der Moment, in dem du den DÃ¶ner so komisch neigen und von unten hochschieben musst, wÃ¤hrend du versuchst, die heraustropfende SoÃŸe mit der Papierserviette aufzufangen. Das ist Akrobatik....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Manchmal beiÃŸe ich auch extra unten eine kleine Ecke ab und sauge die SoÃŸe raus, bevor die Katastrophe passiert....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=16, content='\u001b[33mIst das komisch?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ist das komisch?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Manchmal beiÃŸe ich auch extra unten eine kleine Ecke ab und sauge die SoÃŸe raus, bevor die Katastrophe passiert. Ist das komisch?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=193, content='\u001b[33mOh ja! Der Moment, in dem du den DÃ¶ner so komisch neigen und von unten hochschieben musst, wÃ¤hrend du versuchst, die heraustropfende SoÃŸe mit der Papierserviette aufzufangen. Das ist Akrobatik....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=129, content='\u001b[33mManchmal beiÃŸe ich auch extra unten eine kleine Ecke ab und sauge die SoÃŸe raus, bevor die Katastrophe passiert. Ist das komisch?...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 29760306\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=193:\u001b[33m Oh ja! Der Moment, in dem du den DÃ¶ner so komisch neigen und von unten hochschieben musst, wÃ¤hrend du versuchst, die heraustropfende SoÃŸe mit der Papierserviette aufzufangen. Das ist Akrobatik.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=566886934).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=129:\u001b[33m Manchmal beiÃŸe ich auch extra unten eine kleine Ecke ab und sauge die SoÃŸe raus, bevor die Katastrophe passiert. Ist das komisch?\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=566896941).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  19% 189/1000 [00:13<00:54, 14.91it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  19% 189/1000 [00:13<00:58, 13.79it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=7.560s: temp/gen1_chunk_001_cand_1_try1_seed566896941.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  30% 305/1000 [00:21<00:52, 13.35it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  30% 305/1000 [00:21<00:49, 14.08it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=12.200s: temp/gen1_chunk_000_cand_1_try1_seed566886934.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed566886934.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed566896941.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_053440_456_gen1_seed29760306.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.46 seconds (0:00:00)                                         AM    | 63.7%  ETA 05:34 AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_053440_456_gen1_seed29760306_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_053440_456_gen1_seed29760306.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_053440_456_gen1_seed29760306.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_053440_456_gen1_seed29760306.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_053440_456_gen1_seed29760306.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Warte, echt jetzt? Du saugst die SoÃŸe unten raus? Das ist ja genial! Das ist Next-Level-DÃ¶ner-Konsum. Das ist ja quasi ein eingebauter Strohhalm. Alter, das muss ich ausprobieren! Ich kÃ¤mpfe da immer mit der Schwerkraft und verliere meistens.'\n",
      "\u001b[32m[DEBUG] Split text into 7 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=18, content='\u001b[33mWarte, echt jetzt?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Warte, echt jetzt?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=30, content='\u001b[33mDu saugst die SoÃŸe unten raus?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Du saugst die SoÃŸe unten raus?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=18, content='\u001b[33mDas ist ja genial!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist ja genial!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=32, content='\u001b[33mDas ist Next-Level-DÃ¶ner-Konsum....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist Next-Level-DÃ¶ner-Konsum....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=43, content='\u001b[33mDas ist ja quasi ein eingebauter Strohhalm....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist ja quasi ein eingebauter Strohhalm....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=33, content='\u001b[33mAlter, das muss ich ausprobieren!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Alter, das muss ich ausprobieren!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=62, content='\u001b[33mIch kÃ¤mpfe da immer mit der Schwerkraft und verliere meistens....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ich kÃ¤mpfe da immer mit der Schwerkraft und verliere meistens....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Warte, echt jetzt? Du saugst die SoÃŸe unten raus? Das ist ja genial! Das ist Next-Level-DÃ¶ner-Konsum. Das ist ja quasi ein eingebauter Strohhalm. Alter, das muss ich ausprobieren! Ich kÃ¤mpfe da immer mit der Schwerkraft und verliere meistens....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=242, content='\u001b[33mWarte, echt jetzt? Du saugst die SoÃŸe unten raus? Das ist ja genial! Das ist Next-Level-DÃ¶ner-Konsum. Das ist ja quasi ein eingebauter Strohhalm. Alter, das muss ich ausprobieren! Ich kÃ¤mpfe da immer mit der Schwerkraft und verliere meistens....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3718276357\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=242:\u001b[33m Warte, echt jetzt? Du saugst die SoÃŸe unten raus? Das ist ja genial! Das ist Next-Level-DÃ¶ner-Konsum. Das ist ja quasi ein eingebauter Strohhalm. Alter, das muss ich ausprobieren! Ich kÃ¤mpfe da immer mit der Schwerkraft und verliere meistens.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=1179695695).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  34% 339/1000 [00:25<00:49, 13.47it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=13.560s: temp/gen1_chunk_000_cand_1_try1_seed1179695695.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed1179695695.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_053525_502_gen1_seed3718276357.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.39 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_053525_502_gen1_seed3718276357_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_053525_502_gen1_seed3718276357.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_053525_502_gen1_seed3718276357.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_053525_502_gen1_seed3718276357.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_053525_502_gen1_seed3718276357.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Ja, also, nicht wie ein Staubsauger, aber... du weiÃŸt schon! Man... man... ach, wie erklÃ¤r ich das... man verhindert quasi den Staudammbruch, indem man den Druck ablÃ¤sst! Das ist reine Physik, Max!'\n",
      "\u001b[32m[DEBUG] Split text into 3 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=60, content='\u001b[33mJa, also, nicht wie ein Staubsauger, aber... du weiÃŸt schon!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ja, also, nicht wie ein Staubsauger, aber... du weiÃŸt schon!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=109, content='\u001b[33mMan... man... ach, wie erklÃ¤r ich das... man verhindert quasi den Staudammbruch, indem man den Druck ablÃ¤sst!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Man... man... ach, wie erklÃ¤r ich das... man verhindert quasi den Staudammbruch, indem man den Druck ablÃ¤sst!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=26, content='\u001b[33mDas ist reine Physik, Max!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist reine Physik, Max!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Ja, also, nicht wie ein Staubsauger, aber... du weiÃŸt schon! Man... man... ach, wie erklÃ¤r ich das... man verhindert quasi den Staudammbruch, indem man den Druck ablÃ¤sst! Das ist reine Physik, Max!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=197, content='\u001b[33mJa, also, nicht wie ein Staubsauger, aber... du weiÃŸt schon! Man... man... ach, wie erklÃ¤r ich das... man verhindert quasi den Staudammbruch, indem man den Druck ablÃ¤sst! Das ist reine Physik, Max!...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2246357805\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=197:\u001b[33m Ja, also, nicht wie ein Staubsauger, aber... du weiÃŸt schon! Man... man... ach, wie erklÃ¤r ich das... man verhindert quasi den Staudammbruch, indem man den Druck ablÃ¤sst! Das ist reine Physik, Max!\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2158984903).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  27% 268/1000 [00:20<00:51, 14.35it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  27% 268/1000 [00:20<00:55, 13.28it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=10.720s: temp/gen1_chunk_000_cand_1_try1_seed2158984903.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2158984903.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_053608_019_gen1_seed2246357805.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.26 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_053608_019_gen1_seed2246357805_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_053608_019_gen1_seed2246357805.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_053608_019_gen1_seed2246357805.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_053608_019_gen1_seed2246357805.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_053608_019_gen1_seed2246357805.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Du bist ein DÃ¶ner-Ingenieur, Anna! Das ist krass. Aber zurÃ¼ck zum Brot. Was ich auch liebe, ist, wenn das Brot kurz auf dem Kontaktgrill war. Nur so fÃ¼r 30 Sekunden, damit es diese leichten RÃ¶ststreifen kriegt und nochmal extra warm und knusprig wird. Das macht einen riesigen Unterschied.'\n",
      "\u001b[32m[DEBUG] Split text into 6 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=34, content='\u001b[33mDu bist ein DÃ¶ner-Ingenieur, Anna!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Du bist ein DÃ¶ner-Ingenieur, Anna!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=14, content='\u001b[33mDas ist krass....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist krass....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=21, content='\u001b[33mAber zurÃ¼ck zum Brot....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Aber zurÃ¼ck zum Brot....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=69, content='\u001b[33mWas ich auch liebe, ist, wenn das Brot kurz auf dem Kontaktgrill war....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Was ich auch liebe, ist, wenn das Brot kurz auf dem Kontaktgrill war....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=109, content='\u001b[33mNur so fÃ¼r 30 Sekunden, damit es diese leichten RÃ¶ststreifen kriegt und nochmal extra warm und knusprig wird....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Nur so fÃ¼r 30 Sekunden, damit es diese leichten RÃ¶ststreifen kriegt und nochmal extra warm und knusprig wird....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=37, content='\u001b[33mDas macht einen riesigen Unterschied....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das macht einen riesigen Unterschied....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Du bist ein DÃ¶ner-Ingenieur, Anna! Das ist krass. Aber zurÃ¼ck zum Brot. Was ich auch liebe, ist, wenn das Brot kurz auf dem Kontaktgrill war. Nur so fÃ¼r 30 Sekunden, damit es diese leichten RÃ¶ststreifen kriegt und nochmal extra warm und knusprig wird. Das macht einen riesigen Unterschied....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=289, content='\u001b[33mDu bist ein DÃ¶ner-Ingenieur, Anna! Das ist krass. Aber zurÃ¼ck zum Brot. Was ich auch liebe, ist, wenn das Brot kurz auf dem Kontaktgrill war. Nur so fÃ¼r 30 Sekunden, damit es diese leichten RÃ¶ststreifen kriegt und nochmal extra warm und knusprig wird. Das macht einen riesigen Unterschied....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 4209886850\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=289:\u001b[33m Du bist ein DÃ¶ner-Ingenieur, Anna! Das ist krass. Aber zurÃ¼ck zum Brot. Was ich auch liebe, ist, wenn das Brot kurz auf dem Kontaktgrill war. Nur so fÃ¼r 30 Sekunden, damit es diese leichten RÃ¶ststreifen kriegt und nochmal extra warm und knusprig wird. Das macht einen riesigen Unterschied.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2600892422).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  44% 443/1000 [00:33<00:42, 13.21it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  44% 443/1000 [00:33<00:42, 13.06it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=17.720s: temp/gen1_chunk_000_cand_1_try1_seed2600892422.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2600892422.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_053701_639_gen1_seed4209886850.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.44 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_053701_639_gen1_seed4209886850_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_053701_639_gen1_seed4209886850.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_053701_639_gen1_seed4209886850.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_053701_639_gen1_seed4209886850.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_053701_639_gen1_seed4209886850.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Ja, auf jeden Fall! Das ist der Premium-Move. Wenn der DÃ¶nermann das Brot toastet, weiÃŸt du, er meint es ernst. Er hat Respekt vor seinem Handwerk. Und vor dir, dem Kunden. Das ist wie eine kleine LiebeserklÃ¤rung.'\n",
      "\u001b[32m[DEBUG] Split text into 6 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=19, content='\u001b[33mJa, auf jeden Fall!...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ja, auf jeden Fall!...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=25, content='\u001b[33mDas ist der Premium-Move....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist der Premium-Move....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=65, content='\u001b[33mWenn der DÃ¶nermann das Brot toastet, weiÃŸt du, er meint es ernst....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Wenn der DÃ¶nermann das Brot toastet, weiÃŸt du, er meint es ernst....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=35, content='\u001b[33mEr hat Respekt vor seinem Handwerk....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Er hat Respekt vor seinem Handwerk....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=24, content='\u001b[33mUnd vor dir, dem Kunden....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und vor dir, dem Kunden....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=40, content='\u001b[33mDas ist wie eine kleine LiebeserklÃ¤rung....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist wie eine kleine LiebeserklÃ¤rung....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Ja, auf jeden Fall! Das ist der Premium-Move. Wenn der DÃ¶nermann das Brot toastet, weiÃŸt du, er meint es ernst. Er hat Respekt vor seinem Handwerk. Und vor dir, dem Kunden. Das ist wie eine kleine LiebeserklÃ¤rung....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=213, content='\u001b[33mJa, auf jeden Fall! Das ist der Premium-Move. Wenn der DÃ¶nermann das Brot toastet, weiÃŸt du, er meint es ernst. Er hat Respekt vor seinem Handwerk. Und vor dir, dem Kunden. Das ist wie eine kleine LiebeserklÃ¤rung....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2397192928\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=213:\u001b[33m Ja, auf jeden Fall! Das ist der Premium-Move. Wenn der DÃ¶nermann das Brot toastet, weiÃŸt du, er meint es ernst. Er hat Respekt vor seinem Handwerk. Und vor dir, dem Kunden. Das ist wie eine kleine LiebeserklÃ¤rung.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2778022048).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  31% 308/1000 [00:22<00:49, 14.01it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  31% 309/1000 [00:23<00:51, 13.40it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=12.360s: temp/gen1_chunk_000_cand_1_try1_seed2778022048.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2778022048.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_053743_741_gen1_seed2397192928.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.32 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_053743_741_gen1_seed2397192928_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_053743_741_gen1_seed2397192928.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_053743_741_gen1_seed2397192928.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_053743_741_gen1_seed2397192928.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_053743_741_gen1_seed2397192928.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Okay, SÃ¤ule eins, Brot, haben wir. Knusprig, warm, stabil, am besten selbstgebacken und kurz getoastet. Check. Kommen wir zu SÃ¤ule zwei: Das Fleisch. Oh, das ist ein Minenfeld. Da trennt sich die Spreu vom Weizen, oder?'\n",
      "\u001b[32m[DEBUG] Split text into 6 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=34, content='\u001b[33mOkay, SÃ¤ule eins, Brot, haben wir....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Okay, SÃ¤ule eins, Brot, haben wir....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=68, content='\u001b[33mKnusprig, warm, stabil, am besten selbstgebacken und kurz getoastet....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Knusprig, warm, stabil, am besten selbstgebacken und kurz getoastet....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=6, content='\u001b[33mCheck....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Check....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=38, content='\u001b[33mKommen wir zu SÃ¤ule zwei: Das Fleisch....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Kommen wir zu SÃ¤ule zwei: Das Fleisch....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=26, content='\u001b[33mOh, das ist ein Minenfeld....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Oh, das ist ein Minenfeld....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=42, content='\u001b[33mDa trennt sich die Spreu vom Weizen, oder?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Da trennt sich die Spreu vom Weizen, oder?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Okay, SÃ¤ule eins, Brot, haben wir. Knusprig, warm, stabil, am besten selbstgebacken und kurz getoastet. Check. Kommen wir zu SÃ¤ule zwei: Das Fleisch. Oh, das ist ein Minenfeld. Da trennt sich die Spreu vom Weizen, oder?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=219, content='\u001b[33mOkay, SÃ¤ule eins, Brot, haben wir. Knusprig, warm, stabil, am besten selbstgebacken und kurz getoastet. Check. Kommen wir zu SÃ¤ule zwei: Das Fleisch. Oh, das ist ein Minenfeld. Da trennt sich die Spreu vom Weizen, oder?...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2387481364\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=219:\u001b[33m Okay, SÃ¤ule eins, Brot, haben wir. Knusprig, warm, stabil, am besten selbstgebacken und kurz getoastet. Check. Kommen wir zu SÃ¤ule zwei: Das Fleisch. Oh, das ist ein Minenfeld. Da trennt sich die Spreu vom Weizen, oder?\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=2105943612).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  35% 354/1000 [00:27<00:47, 13.55it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  35% 354/1000 [00:27<00:50, 12.85it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=14.160s: temp/gen1_chunk_000_cand_1_try1_seed2105943612.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed2105943612.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_053830_136_gen1_seed2387481364.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.33 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_053830_136_gen1_seed2387481364_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_053830_136_gen1_seed2387481364.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_053830_136_gen1_seed2387481364.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_053830_136_gen1_seed2387481364.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_053830_136_gen1_seed2387481364.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Oh ja. Das ist der Moment der Wahrheit. Du schaust dir diesen SpieÃŸ an und du weiÃŸt sofort Bescheid. Ist das ein echter Yaprak-DÃ¶ner, also aus aufgeschichteten Fleischscheiben? Oder ist das dieser... dieser Pressfleisch-SpieÃŸ aus Hackfleischmasse?'\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=6, content='\u001b[33mOh ja....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Oh ja....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=32, content='\u001b[33mDas ist der Moment der Wahrheit....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist der Moment der Wahrheit....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=60, content='\u001b[33mDu schaust dir diesen SpieÃŸ an und du weiÃŸt sofort Bescheid....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Du schaust dir diesen SpieÃŸ an und du weiÃŸt sofort Bescheid....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=75, content='\u001b[33mIst das ein echter Yaprak-DÃ¶ner, also aus aufgeschichteten Fleischscheiben?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ist das ein echter Yaprak-DÃ¶ner, also aus aufgeschichteten Fleischscheiben?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=70, content='\u001b[33mOder ist das dieser... dieser Pressfleisch-SpieÃŸ aus Hackfleischmasse?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Oder ist das dieser... dieser Pressfleisch-SpieÃŸ aus Hackfleischmasse?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Oh ja. Das ist der Moment der Wahrheit. Du schaust dir diesen SpieÃŸ an und du weiÃŸt sofort Bescheid. Ist das ein echter Yaprak-DÃ¶ner, also aus aufgeschichteten Fleischscheiben? Oder ist das dieser... dieser Pressfleisch-SpieÃŸ aus Hackfleischmasse?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=247, content='\u001b[33mOh ja. Das ist der Moment der Wahrheit. Du schaust dir diesen SpieÃŸ an und du weiÃŸt sofort Bescheid. Ist das ein echter Yaprak-DÃ¶ner, also aus aufgeschichteten Fleischscheiben? Oder ist das dieser... dieser Pressfleisch-SpieÃŸ aus Hackfleischmasse?...'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3804431952\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=247:\u001b[33m Oh ja. Das ist der Moment der Wahrheit. Du schaust dir diesen SpieÃŸ an und du weiÃŸt sofort Bescheid. Ist das ein echter Yaprak-DÃ¶ner, also aus aufgeschichteten Fleischscheiben? Oder ist das dieser... dieser Pressfleisch-SpieÃŸ aus Hackfleischmasse?\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=4284172016).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  32% 318/1000 [00:24<00:49, 13.71it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  32% 319/1000 [00:24<00:52, 13.06it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=12.760s: temp/gen1_chunk_000_cand_1_try1_seed4284172016.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed4284172016.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_053913_243_gen1_seed3804431952.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.3 seconds (0:00:00)                                          AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_053913_243_gen1_seed3804431952_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_053913_243_gen1_seed3804431952.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_053913_243_gen1_seed3804431952.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_053913_243_gen1_seed3804431952.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_053913_243_gen1_seed3804431952.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: 'Boah, hÃ¶r auf mit dem Pressfleisch-SpieÃŸ. Das ist der Endgegner. Das sieht schon so unnatÃ¼rlich aus, so eine homogene, graubraune Masse. Und wenn die das abschneiden, sind das nicht so schÃ¶ne, faserige StÃ¼cke, sondern so... kleine, brÃ¶selige KrÃ¼mel. Das ist einfach traurig.'\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=41, content='\u001b[33mBoah, hÃ¶r auf mit dem Pressfleisch-SpieÃŸ....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Boah, hÃ¶r auf mit dem Pressfleisch-SpieÃŸ....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=22, content='\u001b[33mDas ist der Endgegner....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist der Endgegner....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=71, content='\u001b[33mDas sieht schon so unnatÃ¼rlich aus, so eine homogene, graubraune Masse....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das sieht schon so unnatÃ¼rlich aus, so eine homogene, graubraune Masse....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=112, content='\u001b[33mUnd wenn die das abschneiden, sind das nicht so schÃ¶ne, faserige StÃ¼cke, sondern so... kleine, brÃ¶selige KrÃ¼mel....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und wenn die das abschneiden, sind das nicht so schÃ¶ne, faserige StÃ¼cke, sondern so... kleine, brÃ¶selige KrÃ¼mel....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=24, content='\u001b[33mDas ist einfach traurig....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist einfach traurig....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Boah, hÃ¶r auf mit dem Pressfleisch-SpieÃŸ. Das ist der Endgegner. Das sieht schon so unnatÃ¼rlich aus, so eine homogene, graubraune Masse. Und wenn die das abschneiden, sind das nicht so schÃ¶ne, faserige StÃ¼cke, sondern so... kleine, brÃ¶selige KrÃ¼mel. Das ist einfach traurig....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=274, content='\u001b[33mBoah, hÃ¶r auf mit dem Pressfleisch-SpieÃŸ. Das ist der Endgegner. Das sieht schon so unnatÃ¼rlich aus, so eine homogene, graubraune Masse. Und wenn die das abschneiden, sind das nicht so schÃ¶ne, faserige StÃ¼cke, sondern so... kleine, brÃ¶selige KrÃ¼mel. Das ist einfach traurig....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2352854019\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=274:\u001b[33m Boah, hÃ¶r auf mit dem Pressfleisch-SpieÃŸ. Das ist der Endgegner. Das sieht schon so unnatÃ¼rlich aus, so eine homogene, graubraune Masse. Und wenn die das abschneiden, sind das nicht so schÃ¶ne, faserige StÃ¼cke, sondern so... kleine, brÃ¶selige KrÃ¼mel. Das ist einfach traurig.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=683401929).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  44% 445/1000 [00:35<00:41, 13.43it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  44% 445/1000 [00:35<00:44, 12.57it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=17.800s: temp/gen1_chunk_000_cand_1_try1_seed683401929.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed683401929.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_054008_913_gen1_seed2352854019.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.43 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_054008_913_gen1_seed2352854019_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_054008_913_gen1_seed2352854019.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_054008_913_gen1_seed2352854019.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_054008_913_gen1_seed2352854019.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_054008_913_gen1_seed2352854019.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Genau. Ein guter SpieÃŸ hat Struktur. Du siehst die einzelnen Fleischschichten, die mariniert wurden. Du siehst, dass da oben vielleicht noch eine Tomate oder eine Paprika draufsteckt, die ihren Saft langsam nach unten abgibt. Und wenn das Fleisch abgeschnitten wird, muss es an den RÃ¤ndern schÃ¶n knusprig und dunkel sein, aber innen noch saftig.'\n",
      "\u001b[32m[DEBUG] Split text into 5 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=6, content='\u001b[33mGenau....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Genau....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=29, content='\u001b[33mEin guter SpieÃŸ hat Struktur....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ein guter SpieÃŸ hat Struktur....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=63, content='\u001b[33mDu siehst die einzelnen Fleischschichten, die mariniert wurden....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Du siehst die einzelnen Fleischschichten, die mariniert wurden....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=124, content='\u001b[33mDu siehst, dass da oben vielleicht noch eine Tomate oder eine Paprika draufsteckt, die ihren Saft langsam nach unten abgibt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Du siehst, dass da oben vielleicht noch eine Tomate oder eine Paprika draufsteckt, die ihren Saft langsam nach unten abgibt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=119, content='\u001b[33mUnd wenn das Fleisch abgeschnitten wird, muss es an den RÃ¤ndern schÃ¶n knusprig und dunkel sein, aber innen noch saftig....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Genau. Ein guter SpieÃŸ hat Struktur. Du siehst die einzelnen Fleischschichten, die mariniert wurden. Du siehst, dass da oben vielleicht noch eine Tomate oder eine Paprika draufsteckt, die ihren Saft langsam nach unten abgibt....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Und wenn das Fleisch abgeschnitten wird, muss es an den RÃ¤ndern schÃ¶n knusprig und dunkel sein, aber innen noch saftig....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Und wenn das Fleisch abgeschnitten wird, muss es an den RÃ¤ndern schÃ¶n knusprig und dunkel sein, aber innen noch saftig....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=225, content='\u001b[33mGenau. Ein guter SpieÃŸ hat Struktur. Du siehst die einzelnen Fleischschichten, die mariniert wurden. Du siehst, dass da oben vielleicht noch eine Tomate oder eine Paprika draufsteckt, die ihren Saft langsam nach unten abgibt....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=119, content='\u001b[33mUnd wenn das Fleisch abgeschnitten wird, muss es an den RÃ¤ndern schÃ¶n knusprig und dunkel sein, aber innen noch saftig....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 3410933099\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=225:\u001b[33m Genau. Ein guter SpieÃŸ hat Struktur. Du siehst die einzelnen Fleischschichten, die mariniert wurden. Du siehst, dass da oben vielleicht noch eine Tomate oder eine Paprika draufsteckt, die ihren Saft langsam nach unten abgibt.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=564400385).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=119:\u001b[33m Und wenn das Fleisch abgeschnitten wird, muss es an den RÃ¤ndern schÃ¶n knusprig und dunkel sein, aber innen noch saftig.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=564410392).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  30% 296/1000 [00:22<00:51, 13.73it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  30% 297/1000 [00:22<00:54, 12.98it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=11.880s: temp/gen1_chunk_000_cand_1_try1_seed564400385.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  17% 173/1000 [00:12<00:57, 14.31it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  17% 174/1000 [00:12<01:01, 13.48it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=6.960s: temp/gen1_chunk_001_cand_1_try1_seed564410392.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed564400385.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed564410392.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_054107_003_gen1_seed3410933099.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.42 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_054107_003_gen1_seed3410933099_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_054107_003_gen1_seed3410933099.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_054107_003_gen1_seed3410933099.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_054107_003_gen1_seed3410933099.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_054107_003_gen1_seed3410933099.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/2a6f92185ea867f0579af327fa64ebf438f351ea83290178e0cda6a59be0f958/max.wav'\n",
      "[DEBUG] After reference number removal: \"Und dann die groÃŸe Frage: Kalb, Lamm oder HÃ¤hnchen? Was ist dein Favorit? In Hamburg gibt's ja fast nur noch HÃ¤hnchen-DÃ¶ner, habe ich das GefÃ¼hl.\"\n",
      "\u001b[32m[DEBUG] Split text into 3 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=51, content='\u001b[33mUnd dann die groÃŸe Frage: Kalb, Lamm oder HÃ¤hnchen?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Und dann die groÃŸe Frage: Kalb, Lamm oder HÃ¤hnchen?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=21, content='\u001b[33mWas ist dein Favorit?...'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Was ist dein Favorit?...\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=71, content='\u001b[33mIn Hamburg gibt's ja fast nur noch HÃ¤hnchen-DÃ¶ner, habe ich das GefÃ¼hl....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: In Hamburg gibt's ja fast nur noch HÃ¤hnchen-DÃ¶ner, habe ich das GefÃ¼hl....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Und dann die groÃŸe Frage: Kalb, Lamm oder HÃ¤hnchen? Was ist dein Favorit? In Hamburg gibt's ja fast nur noch HÃ¤hnchen-DÃ¶ner, habe ich das GefÃ¼hl....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 1\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=145, content='\u001b[33mUnd dann die groÃŸe Frage: Kalb, Lamm oder HÃ¤hnchen? Was ist dein Favorit? In Hamburg gibt's ja fast nur noch HÃ¤hnchen-DÃ¶ner, habe ich das GefÃ¼hl....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 1631192831\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=145:\u001b[33m Und dann die groÃŸe Frage: Kalb, Lamm oder HÃ¤hnchen? Was ist dein Favorit? In Hamburg gibt's ja fast nur noch HÃ¤hnchen-DÃ¶ner, habe ich das GefÃ¼hl.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=3505296061).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  27% 269/1000 [00:19<00:51, 14.20it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  27% 270/1000 [00:19<00:52, 13.78it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=10.800s: temp/gen1_chunk_000_cand_1_try1_seed3505296061.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/1 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed3505296061.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_054145_170_gen1_seed1631192831.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.31 seconds (0:00:00)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_054145_170_gen1_seed1631192831_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_054145_170_gen1_seed1631192831.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_054145_170_gen1_seed1631192831.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_054145_170_gen1_seed1631192831.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_054145_170_gen1_seed1631192831.mp3\n",
      "[DEBUG] Received audio_prompt_path_input: '/tmp/gradio/f0f82cef36a364d098c977f51d4425644f6fcfa4f32c8a39b35b7d6b3c037bc7/anna.wav'\n",
      "[DEBUG] After reference number removal: 'Also, der absolute Klassiker ist fÃ¼r mich Kalb. Ein richtig guter KalbsdÃ¶ner, gut gewÃ¼rzt, das ist einfach unschlagbar. Das ist der DÃ¶ner, an den ich denke, wenn ich \"DÃ¶ner\" sage. HÃ¤hnchen-DÃ¶ner kann aber auch super sein, wenn er gut gemacht ist. Dann ist er oft ein bisschen leichter, nicht ganz so wuchtig. Das ist dann der DÃ¶ner fÃ¼r den Sommer-Nachmittag.'\n",
      "\u001b[32m[DEBUG] Split text into 6 sentences.\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=47, content='\u001b[33mAlso, der absolute Klassiker ist fÃ¼r mich Kalb....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Also, der absolute Klassiker ist fÃ¼r mich Kalb....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=71, content='\u001b[33mEin richtig guter KalbsdÃ¶ner, gut gewÃ¼rzt, das ist einfach unschlagbar....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Ein richtig guter KalbsdÃ¶ner, gut gewÃ¼rzt, das ist einfach unschlagbar....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=59, content='\u001b[33mDas ist der DÃ¶ner, an den ich denke, wenn ich \"DÃ¶ner\" sage....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist der DÃ¶ner, an den ich denke, wenn ich \"DÃ¶ner\" sage....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=66, content='\u001b[33mHÃ¤hnchen-DÃ¶ner kann aber auch super sein, wenn er gut gemacht ist....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: HÃ¤hnchen-DÃ¶ner kann aber auch super sein, wenn er gut gemacht ist....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=61, content='\u001b[33mDann ist er oft ein bisschen leichter, nicht ganz so wuchtig....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized chunk: Also, der absolute Klassiker ist fÃ¼r mich Kalb. Ein richtig guter KalbsdÃ¶ner, gut gewÃ¼rzt, das ist einfach unschlagbar. Das ist der DÃ¶ner, an den ich denke, wenn ich \"DÃ¶ner\" sage. HÃ¤hnchen-DÃ¶ner kann aber auch super sein, wenn er gut gemacht ist....\u001b[0m\n",
      "\u001b[32m[DEBUG] Starting new chunk with: Dann ist er oft ein bisschen leichter, nicht ganz so wuchtig....\u001b[0m\n",
      "\u001b[32m[DEBUG] Processing sentence: len=49, content='\u001b[33mDas ist dann der DÃ¶ner fÃ¼r den Sommer-Nachmittag....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Adding sentence to chunk: Das ist dann der DÃ¶ner fÃ¼r den Sommer-Nachmittag....\u001b[0m\n",
      "\u001b[32m[DEBUG] Finalized final chunk: Dann ist er oft ein bisschen leichter, nicht ganz so wuchtig. Das ist dann der DÃ¶ner fÃ¼r den Sommer-Nachmittag....\u001b[0m\n",
      "\u001b[32m[DEBUG] Total chunks created: 2\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 0: len=246, content='\u001b[33mAlso, der absolute Klassiker ist fÃ¼r mich Kalb. Ein richtig guter KalbsdÃ¶ner, gut gewÃ¼rzt, das ist einfach unschlagbar. Das ist der DÃ¶ner, an den ich denke, wenn ich \"DÃ¶ner\" sage. HÃ¤hnchen-DÃ¶ner kann aber auch super sein, wenn er gut gemacht ist....'\u001b[0m\n",
      "\u001b[32m[DEBUG] Chunk 1: len=111, content='\u001b[33mDann ist er oft ein bisschen leichter, nicht ganz so wuchtig. Das ist dann der DÃ¶ner fÃ¼r den Sommer-Nachmittag....'\u001b[0m\n",
      "\u001b[43m[DEBUG] Starting generation 1/1 with seed 2351058878\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 0: len=246:\u001b[33m Also, der absolute Klassiker ist fÃ¼r mich Kalb. Ein richtig guter KalbsdÃ¶ner, gut gewÃ¼rzt, das ist einfach unschlagbar. Das ist der DÃ¶ner, an den ich denke, wenn ich \"DÃ¶ner\" sage. HÃ¤hnchen-DÃ¶ner kann aber auch super sein, wenn er gut gemacht ist.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 0 (seed=833346234).\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Processing group 1: len=111:\u001b[33m Dann ist er oft ein bisschen leichter, nicht ganz so wuchtig. Das ist dann der DÃ¶ner fÃ¼r den Sommer-Nachmittag.\u001b[0m\n",
      "\u001b[32m[DEBUG] [DET] Generating cand 1 attempt 1 for chunk 1 (seed=833356241).\u001b[0m\n",
      "/usr/lib/python3.12/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n",
      "/usr/local/lib/python3.12/dist-packages/transformers/generation/configuration_utils.py:774: UserWarning: `return_dict_in_generate` is NOT set to `True`, but `output_attentions` is. When `return_dict_in_generate` is not `True`, `output_attentions` is ignored.\n",
      "  warnings.warn(\n",
      "Sampling:   0% 0/1000 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/transformers/generation/logits_process.py:471: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  cumulative_probs = sorted_logits.softmax(dim=-1).cumsum(dim=-1)\n",
      "Sampling:  81% 814/1000 [01:04<00:14, 13.00it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:ğŸš¨ Detected 2x repetition of token 6486\n",
      "WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(False), alignment_repetition=tensor(False), token_repetition=True\n",
      "Sampling:  81% 814/1000 [01:04<00:14, 12.61it/s]\n",
      "/content/Chatterbox-TTS-Extended/chatterbox-original-multilingual/src/chatterbox/models/s3gen/hifigan.py:211: UserWarning: cumsum_cuda_kernel does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at ../aten/src/ATen/Context.cpp:83.)\n",
      "  theta_mat = 2 * np.pi * (torch.cumsum(F_mat, dim=-1) % 1)\n",
      "/content/Chatterbox-TTS-Extended/Chatter.py:910: FutureWarning: get_duration() keyword argument 'filename' has been renamed to 'path' in version 0.10.0.\n",
      "\tThis alias will be removed in version 1.0.\n",
      "  return librosa.get_duration(filename=path)\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=32.560s: temp/gen1_chunk_000_cand_1_try1_seed833346234.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 1/2 (50%)\u001b[0m\n",
      "Sampling:  15% 149/1000 [00:11<00:58, 14.54it/s]WARNING:chatterbox.models.t3.inference.alignment_stream_analyzer:forcing EOS token, long_tail=tensor(True), alignment_repetition=tensor(False), token_repetition=False\n",
      "Sampling:  15% 149/1000 [00:11<01:03, 13.49it/s]\n",
      "\u001b[32m[DEBUG] [DET] Saved cand 1, attempt 1, duration=5.960s: temp/gen1_chunk_001_cand_1_try1_seed833356241.wav\u001b[0m\n",
      "\u001b[36m[PROGRESS] Generated chunk 2/2 (100%)\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_000_cand_1_try1_seed833346234.wav as shortest candidate for chunk 0\u001b[0m\n",
      "\u001b[32m[DEBUG] [Bypass Whisper] Selected temp/gen1_chunk_001_cand_1_try1_seed833356241.wav as shortest candidate for chunk 1\u001b[0m\n",
      "\u001b[104m[DEBUG] \u001b[5mFinal audio concatenated, output file: output/text_input_audio_2025-12-21_054326_823_gen1_seed2351058878.wav\u001b[0m\n",
      "[WARNING] pyrnnoise not installed; skipping denoise.\n",
      "Finished. took 0.81 seconds (0:00:01)                                         AM    \n",
      "Error: no \"view\" mailcap rules found for type \"audio/x-wav\"\n",
      "/usr/bin/open: 882: www-browser: not found\n",
      "/usr/bin/open: 882: links2: not found\n",
      "/usr/bin/open: 882: elinks: not found\n",
      "/usr/bin/open: 882: links: not found\n",
      "/usr/bin/open: 882: lynx: not found\n",
      "/usr/bin/open: 882: w3m: not found\n",
      "xdg-open: no method available for opening 'output/text_input_audio_2025-12-21_054326_823_gen1_seed2351058878_cleaned.wav'\n",
      "\u001b[32m[DEBUG] Post-processed with auto-editor: output/text_input_audio_2025-12-21_054326_823_gen1_seed2351058878.wav\u001b[0m\n",
      "\u001b[32m[DEBUG] Post-processed with ffmpeg normalization: output/text_input_audio_2025-12-21_054326_823_gen1_seed2351058878.wav\u001b[0m\n",
      "[SPEED] Applied atempo=0.95 to: output/text_input_audio_2025-12-21_054326_823_gen1_seed2351058878.wav\n",
      "\u001b[1;36m[DEBUG] \u001b[6;4;3;34;102mALL GENERATIONS COMPLETE. Outputs:\u001b[0m\n",
      "output/text_input_audio_2025-12-21_054326_823_gen1_seed2351058878.mp3\n",
      "Keyboard interruption in main thread... closing server.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/gradio/blocks.py\", line 3043, in block_thread\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Launch the Gradio interface\n",
    "print(\"ğŸš€ Launching Chatterbox-TTS-Extended...\\n\")\n",
    "print(\"â³ First generation will take longer as models download and load.\")\n",
    "print(\"ğŸ“Š Monitor the output below for progress updates.\\n\")\n",
    "\n",
    "# Run with public sharing enabled for Colab\n",
    "!python Chatter.py --share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommended-settings"
   },
   "source": [
    "## âš™ï¸ Recommended Settings for Colab\n",
    "\n",
    "### For Free Tier (T4 GPU, ~15GB VRAM):\n",
    "```\n",
    "Whisper Model: tiny or base\n",
    "Use faster-whisper: âœ… Enabled\n",
    "Candidates per chunk: 2-3\n",
    "Parallel workers: 2-3\n",
    "Enable RNNoise: âœ… Enabled (removes artifacts!)\n",
    "```\n",
    "\n",
    "### For Pro/Pro+ (A100/V100, more VRAM):\n",
    "```\n",
    "Whisper Model: small or medium\n",
    "Use faster-whisper: âœ… Enabled\n",
    "Candidates per chunk: 3-5\n",
    "Parallel workers: 4-6\n",
    "Enable RNNoise: âœ… Enabled\n",
    "```\n",
    "\n",
    "### To Reduce Artifacts (Main Goal!):\n",
    "1. **Enable RNNoise denoising** - This is the key feature!\n",
    "2. Use **3+ candidates per chunk** with Whisper validation\n",
    "3. Enable **Auto-Editor** for cleanup\n",
    "4. Use **faster-whisper** for efficient validation\n",
    "5. Set **Max Attempts to 3** to retry failed chunks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## ğŸ”§ Troubleshooting\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "#### 1. **CUDA Out of Memory Error**\n",
    "```python\n",
    "# Solution: Restart runtime and reduce settings\n",
    "# Runtime â†’ Restart runtime\n",
    "# Then use these settings:\n",
    "# - Whisper model: tiny\n",
    "# - Candidates: 1-2\n",
    "# - Parallel workers: 1\n",
    "```\n",
    "\n",
    "#### 2. **Slow Performance**\n",
    "```python\n",
    "# Make sure GPU is enabled:\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "# If False, go to Runtime â†’ Change runtime type â†’ GPU\n",
    "```\n",
    "\n",
    "#### 3. **Model Download Failures**\n",
    "```python\n",
    "# Retry the cell or check your internet connection\n",
    "# Models are downloaded from Hugging Face on first use\n",
    "```\n",
    "\n",
    "#### 4. **Audio Has Noise/Artifacts**\n",
    "```python\n",
    "# Enable these features in the UI:\n",
    "# âœ… Denoise with RNNoise (pyrnnoise)\n",
    "# âœ… Post-process with Auto-Editor\n",
    "# âœ… Use faster-whisper validation\n",
    "# Increase candidates per chunk to 3-5\n",
    "```\n",
    "\n",
    "#### 5. **Session Timeout**\n",
    "```python\n",
    "# Colab free tier has time limits\n",
    "# Save your audio files regularly\n",
    "# Consider upgrading to Colab Pro for longer sessions\n",
    "```\n",
    "\n",
    "#### 6. **FFmpeg Errors**\n",
    "```python\n",
    "# Reinstall FFmpeg:\n",
    "!apt-get install --reinstall -y ffmpeg\n",
    "```\n",
    "\n",
    "#### 7. **Import Errors**\n",
    "```python\n",
    "# Restart runtime and run all cells in order\n",
    "# Runtime â†’ Restart runtime\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick-test"
   },
   "source": [
    "## ğŸ§ª Quick Test (Optional)\n",
    "\n",
    "Test the installation with a simple command-line generation before launching the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-installation"
   },
   "outputs": [],
   "source": [
    "# Quick test to verify everything is working\n",
    "print(\"ğŸ§ª Testing installation...\\n\")\n",
    "\n",
    "try:\n",
    "    # Test imports\n",
    "    import torch\n",
    "    import torchaudio\n",
    "    import gradio as gr\n",
    "    from chatterbox.src.chatterbox.tts import ChatterboxTTS\n",
    "    \n",
    "    print(\"âœ… Core imports successful\")\n",
    "    print(f\"âœ… PyTorch: {torch.__version__}\")\n",
    "    print(f\"âœ… CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"âœ… Gradio: {gr.__version__}\")\n",
    "    \n",
    "    # Test optional imports\n",
    "    try:\n",
    "        import pyrnnoise\n",
    "        print(\"âœ… pyrnnoise (RNNoise) available - artifact reduction enabled!\")\n",
    "    except:\n",
    "        print(\"âš ï¸  pyrnnoise not available - denoising will be skipped\")\n",
    "    \n",
    "    try:\n",
    "        from faster_whisper import WhisperModel\n",
    "        print(\"âœ… faster-whisper available\")\n",
    "    except:\n",
    "        print(\"âš ï¸  faster-whisper not available\")\n",
    "    \n",
    "    print(\"\\nâœ… Installation test passed! Ready to use.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Installation test failed: {e}\")\n",
    "    print(\"Please run the installation cells again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usage-tips"
   },
   "source": [
    "## ğŸ’¡ Usage Tips\n",
    "\n",
    "### Getting the Best Results:\n",
    "\n",
    "1. **Reference Audio**: Upload a clean 3-10 second sample of the target voice\n",
    "2. **Text Preprocessing**: Enable all text cleanup options\n",
    "3. **Quality Settings**: \n",
    "   - Use 3-5 candidates per chunk\n",
    "   - Enable Whisper validation\n",
    "   - Enable RNNoise denoising\n",
    "4. **Export**: Choose FLAC for best quality or MP3 for smaller files\n",
    "\n",
    "### Saving Your Work:\n",
    "\n",
    "Generated audio files are saved in the `output/` directory. Download them before your session ends:\n",
    "\n",
    "```python\n",
    "# List generated files\n",
    "!ls -lh output/\n",
    "\n",
    "# Download all output files\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "for file in os.listdir('output'):\n",
    "    if file.endswith(('.wav', '.mp3', '.flac')):\n",
    "        files.download(f'output/{file}')\n",
    "```\n",
    "\n",
    "### Managing Memory:\n",
    "\n",
    "```python\n",
    "# Clear GPU memory if needed\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"GPU memory cleared\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-outputs"
   },
   "source": [
    "## ğŸ“¥ Download Generated Audio Files\n",
    "\n",
    "Use this cell to download all generated audio files to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-files"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“ Available output files:\\n\")\n",
    "\n",
    "if os.path.exists('output'):\n",
    "    output_files = [f for f in os.listdir('output') if f.endswith(('.wav', '.mp3', '.flac'))]\n",
    "    \n",
    "    if output_files:\n",
    "        for file in output_files:\n",
    "            print(f\"  - {file}\")\n",
    "        \n",
    "        print(\"\\nğŸ“¥ Downloading files...\")\n",
    "        for file in output_files:\n",
    "            try:\n",
    "                files.download(f'output/{file}')\n",
    "                print(f\"âœ… Downloaded: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Failed to download {file}: {e}\")\n",
    "    else:\n",
    "        print(\"No audio files found. Generate some audio first!\")\n",
    "else:\n",
    "    print(\"Output directory not found. Generate some audio first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clear-memory"
   },
   "source": [
    "## ğŸ§¹ Clear GPU Memory\n",
    "\n",
    "Run this cell if you encounter memory issues or want to free up GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clear-gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"ğŸ§¹ Clearing GPU memory...\\n\")\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Get memory stats\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    \n",
    "    print(f\"GPU Memory Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"GPU Memory Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\nâœ… Memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Additional Resources\n",
    "\n",
    "- **GitHub Repository**: [Chatterbox-TTS-Extended](https://github.com/m-marie1/Chatterbox-TTS-Extended)\n",
    "- **Original Chatterbox**: [Resemble AI Chatterbox](https://github.com/resemble-ai/chatterbox)\n",
    "- **Report Issues**: [GitHub Issues](https://github.com/m-marie1/Chatterbox-TTS-Extended/issues)\n",
    "\n",
    "## ğŸ¤ Credits\n",
    "\n",
    "- **Chatterbox-TTS-Extended**: Extended version with artifact reduction\n",
    "- **Original Chatterbox**: Resemble AI\n",
    "- **RNNoise**: Xiph.Org Foundation\n",
    "- **Whisper**: OpenAI\n",
    "\n",
    "---\n",
    "\n",
    "**Enjoy high-quality, artifact-free speech synthesis! ğŸ‰**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilingual support (updated)\n",
    "- Use **Model Variant** â†’ `multilingual_23lang` for non-English; keep `extended_en` for English-only.\n",
    "- Set **Language ID** to a 2-letter code (examples: en, de, fr, es, zh, ja, ko). Defaults to `en` if blank.\n",
    "- All quality features remain (RNNoise, Whisper validation, Auto-Editor, batching, retries).\n",
    "- Weights download on first multilingual use; provide an HF token if your account requires access.\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
