{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# üéß Chatterbox-TTS-Extended on Google Colab\n",
    "\n",
    "## Advanced Text-to-Speech with Artifact Reduction\n",
    "\n",
    "This notebook allows you to run Chatterbox-TTS-Extended on Google Colab, leveraging free GPU resources for high-quality speech synthesis with built-in artifact reduction.\n",
    "\n",
    "### Features:\n",
    "- üé§ High-quality voice cloning and TTS\n",
    "- üîß Advanced artifact reduction with RNNoise\n",
    "- üéØ Whisper-based quality validation\n",
    "- üé® Voice conversion capabilities\n",
    "- üì¶ Multiple export formats (WAV, MP3, FLAC)\n",
    "\n",
    "### Requirements:\n",
    "- Google Colab account (free tier works!)\n",
    "- GPU runtime (recommended: T4 or better)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup-header"
   },
   "source": [
    "## üìã Step 1: Environment Setup\n",
    "\n",
    "**IMPORTANT**: Make sure you have enabled GPU runtime:\n",
    "1. Go to `Runtime` ‚Üí `Change runtime type`\n",
    "2. Select `GPU` as Hardware accelerator\n",
    "3. Choose `T4` GPU (or better if available)\n",
    "4. Click `Save`\n",
    "\n",
    "This cell will:\n",
    "- Check GPU availability\n",
    "- Verify Python version\n",
    "- Display system information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check-environment"
   },
   "outputs": [],
   "source": [
    "# Check GPU and environment\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "print(\"üîç Checking environment...\\n\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python executable: {sys.executable}\")\n",
    "\n",
    "# Check GPU\n",
    "try:\n",
    "    gpu_info = subprocess.check_output(['nvidia-smi'], encoding='utf-8')\n",
    "    print(\"\\n‚úÖ GPU detected:\")\n",
    "    print(gpu_info)\n",
    "except:\n",
    "    print(\"\\n‚ö†Ô∏è  WARNING: No GPU detected. This will be VERY slow!\")\n",
    "    print(\"Please enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-system-deps"
   },
   "source": [
    "## üì¶ Step 2: Install System Dependencies\n",
    "\n",
    "Installing FFmpeg and other system-level tools required for audio processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-ffmpeg"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install FFmpeg (required for audio processing)\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq ffmpeg\n",
    "\n",
    "# Verify FFmpeg installation\n",
    "!ffmpeg -version | head -n 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone-repo"
   },
   "source": [
    "## üì• Step 3: Clone Repository\n",
    "\n",
    "Cloning the Chatterbox-TTS-Extended repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "git-clone"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Remove existing directory if present\n",
    "if os.path.exists('Chatterbox-TTS-Extended'):\n",
    "    print(\"üìÅ Removing existing directory...\")\n",
    "    !rm -rf Chatterbox-TTS-Extended\n",
    "\n",
    "# Clone the repository\n",
    "print(\"üì• Cloning Chatterbox-TTS-Extended repository...\")\n",
    "!git clone https://github.com/m-marie1/Chatterbox-TTS-Extended.git\n",
    "\n",
    "# Change to repository directory\n",
    "%cd Chatterbox-TTS-Extended\n",
    "\n",
    "print(\"\\n‚úÖ Repository cloned successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "install-python-deps"
   },
   "source": [
    "## üêç Step 4: Install Python Dependencies\n",
    "\n",
    "Installing all required Python packages. This may take 3-5 minutes.\n",
    "\n",
    "**Note**: We're using Colab-optimized versions to avoid conflicts with pre-installed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install-deps"
   },
   "outputs": [],
   "source": [
    "# Install PyTorch with CUDA support (Colab uses CUDA 12.x)\n",
    "print(\"üîß Installing PyTorch with CUDA support...\")\n",
    "!pip install -q torch==2.7.0 torchaudio==2.7.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install core dependencies\n",
    "print(\"\\nüì¶ Installing core dependencies...\")\n",
    "!pip install -q gradio numpy faster-whisper openai-whisper ffmpeg-python\n",
    "!pip install -q resampy==0.4.3 librosa==0.10.0 soundfile nltk\n",
    "\n",
    "# Install auto-editor for audio cleanup\n",
    "print(\"\\nüé¨ Installing auto-editor...\")\n",
    "!pip install -q auto-editor==27.1.1\n",
    "\n",
    "# Install Hugging Face and model dependencies\n",
    "print(\"\\nü§ó Installing Hugging Face dependencies...\")\n",
    "!pip install -q transformers==4.46.3 diffusers==0.29.0 omegaconf==2.3.0\n",
    "\n",
    "# Install specific model dependencies\n",
    "print(\"\\nüéØ Installing model-specific dependencies...\")\n",
    "!pip install -q resemble-perth==1.0.1 silero-vad==5.1.2 conformer==0.3.2\n",
    "\n",
    "# Install pyrnnoise for artifact reduction\n",
    "print(\"\\nüîá Installing pyrnnoise for noise reduction...\")\n",
    "!pip install -q pyrnnoise==0.3.8\n",
    "\n",
    "# Install s3tokenizer\n",
    "print(\"\\nüî§ Installing s3tokenizer...\")\n",
    "!pip install -q s3tokenizer\n",
    "\n",
    "# Install spaces (for Gradio compatibility)\n",
    "print(\"\\nüöÄ Installing spaces...\")\n",
    "!pip install -q spaces\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed successfully!\")\n",
    "\n",
    "# Verify key installations\n",
    "print(\"\\nüìä Verifying installations...\")\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-nltk"
   },
   "source": [
    "## üìö Step 5: Download NLTK Data\n",
    "\n",
    "Downloading required NLTK tokenizer data for text processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nltk-download"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(\"üìö Downloading NLTK data...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "print(\"‚úÖ NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "launch-ui"
   },
   "source": [
    "## üöÄ Step 6: Launch Chatterbox-TTS-Extended\n",
    "\n",
    "This will start the Gradio interface. The model will be loaded on first use.\n",
    "\n",
    "**Features available:**\n",
    "- **TTS Tab**: Text-to-Speech with advanced options\n",
    "  - Multiple candidate generation for best quality\n",
    "  - Whisper validation to reduce artifacts\n",
    "  - RNNoise denoising for clean audio\n",
    "  - Auto-editor for silence removal\n",
    "  - Batch processing support\n",
    "- **Voice Conversion Tab**: Convert voice to match a reference\n",
    "\n",
    "**Tips for Colab:**\n",
    "- First generation will take longer as models load\n",
    "- Use smaller Whisper models (tiny/base) to save VRAM\n",
    "- Enable \"Use faster-whisper\" for better performance\n",
    "- Start with 1-2 candidates per chunk to avoid OOM errors\n",
    "- If you get CUDA out of memory, restart runtime and reduce settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "launch-app",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Launch the Gradio interface\n",
    "print(\"üöÄ Launching Chatterbox-TTS-Extended...\\n\")\n",
    "print(\"‚è≥ First generation will take longer as models download and load.\")\n",
    "print(\"üìä Monitor the output below for progress updates.\\n\")\n",
    "\n",
    "# Run with public sharing enabled for Colab\n",
    "!python Chatter.py --share"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "recommended-settings"
   },
   "source": [
    "## ‚öôÔ∏è Recommended Settings for Colab\n",
    "\n",
    "### For Free Tier (T4 GPU, ~15GB VRAM):\n",
    "```\n",
    "Whisper Model: tiny or base\n",
    "Use faster-whisper: ‚úÖ Enabled\n",
    "Candidates per chunk: 2-3\n",
    "Parallel workers: 2-3\n",
    "Enable RNNoise: ‚úÖ Enabled (removes artifacts!)\n",
    "```\n",
    "\n",
    "### For Pro/Pro+ (A100/V100, more VRAM):\n",
    "```\n",
    "Whisper Model: small or medium\n",
    "Use faster-whisper: ‚úÖ Enabled\n",
    "Candidates per chunk: 3-5\n",
    "Parallel workers: 4-6\n",
    "Enable RNNoise: ‚úÖ Enabled\n",
    "```\n",
    "\n",
    "### To Reduce Artifacts (Main Goal!):\n",
    "1. **Enable RNNoise denoising** - This is the key feature!\n",
    "2. Use **3+ candidates per chunk** with Whisper validation\n",
    "3. Enable **Auto-Editor** for cleanup\n",
    "4. Use **faster-whisper** for efficient validation\n",
    "5. Set **Max Attempts to 3** to retry failed chunks\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## üîß Troubleshooting\n",
    "\n",
    "### Common Issues and Solutions:\n",
    "\n",
    "#### 1. **CUDA Out of Memory Error**\n",
    "```python\n",
    "# Solution: Restart runtime and reduce settings\n",
    "# Runtime ‚Üí Restart runtime\n",
    "# Then use these settings:\n",
    "# - Whisper model: tiny\n",
    "# - Candidates: 1-2\n",
    "# - Parallel workers: 1\n",
    "```\n",
    "\n",
    "#### 2. **Slow Performance**\n",
    "```python\n",
    "# Make sure GPU is enabled:\n",
    "import torch\n",
    "print(f\"GPU available: {torch.cuda.is_available()}\")\n",
    "# If False, go to Runtime ‚Üí Change runtime type ‚Üí GPU\n",
    "```\n",
    "\n",
    "#### 3. **Model Download Failures**\n",
    "```python\n",
    "# Retry the cell or check your internet connection\n",
    "# Models are downloaded from Hugging Face on first use\n",
    "```\n",
    "\n",
    "#### 4. **Audio Has Noise/Artifacts**\n",
    "```python\n",
    "# Enable these features in the UI:\n",
    "# ‚úÖ Denoise with RNNoise (pyrnnoise)\n",
    "# ‚úÖ Post-process with Auto-Editor\n",
    "# ‚úÖ Use faster-whisper validation\n",
    "# Increase candidates per chunk to 3-5\n",
    "```\n",
    "\n",
    "#### 5. **Session Timeout**\n",
    "```python\n",
    "# Colab free tier has time limits\n",
    "# Save your audio files regularly\n",
    "# Consider upgrading to Colab Pro for longer sessions\n",
    "```\n",
    "\n",
    "#### 6. **FFmpeg Errors**\n",
    "```python\n",
    "# Reinstall FFmpeg:\n",
    "!apt-get install --reinstall -y ffmpeg\n",
    "```\n",
    "\n",
    "#### 7. **Import Errors**\n",
    "```python\n",
    "# Restart runtime and run all cells in order\n",
    "# Runtime ‚Üí Restart runtime\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "quick-test"
   },
   "source": [
    "## üß™ Quick Test (Optional)\n",
    "\n",
    "Test the installation with a simple command-line generation before launching the UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test-installation"
   },
   "outputs": [],
   "source": [
    "# Quick test to verify everything is working\n",
    "print(\"üß™ Testing installation...\\n\")\n",
    "\n",
    "try:\n",
    "    # Test imports\n",
    "    import torch\n",
    "    import torchaudio\n",
    "    import gradio as gr\n",
    "    from chatterbox.src.chatterbox.tts import ChatterboxTTS\n",
    "    \n",
    "    print(\"‚úÖ Core imports successful\")\n",
    "    print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "    print(f\"‚úÖ CUDA available: {torch.cuda.is_available()}\")\n",
    "    print(f\"‚úÖ Gradio: {gr.__version__}\")\n",
    "    \n",
    "    # Test optional imports\n",
    "    try:\n",
    "        import pyrnnoise\n",
    "        print(\"‚úÖ pyrnnoise (RNNoise) available - artifact reduction enabled!\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  pyrnnoise not available - denoising will be skipped\")\n",
    "    \n",
    "    try:\n",
    "        from faster_whisper import WhisperModel\n",
    "        print(\"‚úÖ faster-whisper available\")\n",
    "    except:\n",
    "        print(\"‚ö†Ô∏è  faster-whisper not available\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Installation test passed! Ready to use.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Installation test failed: {e}\")\n",
    "    print(\"Please run the installation cells again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usage-tips"
   },
   "source": [
    "## üí° Usage Tips\n",
    "\n",
    "### Getting the Best Results:\n",
    "\n",
    "1. **Reference Audio**: Upload a clean 3-10 second sample of the target voice\n",
    "2. **Text Preprocessing**: Enable all text cleanup options\n",
    "3. **Quality Settings**: \n",
    "   - Use 3-5 candidates per chunk\n",
    "   - Enable Whisper validation\n",
    "   - Enable RNNoise denoising\n",
    "4. **Export**: Choose FLAC for best quality or MP3 for smaller files\n",
    "\n",
    "### Saving Your Work:\n",
    "\n",
    "Generated audio files are saved in the `output/` directory. Download them before your session ends:\n",
    "\n",
    "```python\n",
    "# List generated files\n",
    "!ls -lh output/\n",
    "\n",
    "# Download all output files\n",
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "for file in os.listdir('output'):\n",
    "    if file.endswith(('.wav', '.mp3', '.flac')):\n",
    "        files.download(f'output/{file}')\n",
    "```\n",
    "\n",
    "### Managing Memory:\n",
    "\n",
    "```python\n",
    "# Clear GPU memory if needed\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "print(\"GPU memory cleared\")\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download-outputs"
   },
   "source": [
    "## üì• Download Generated Audio Files\n",
    "\n",
    "Use this cell to download all generated audio files to your local machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download-files"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "print(\"üìÅ Available output files:\\n\")\n",
    "\n",
    "if os.path.exists('output'):\n",
    "    output_files = [f for f in os.listdir('output') if f.endswith(('.wav', '.mp3', '.flac'))]\n",
    "    \n",
    "    if output_files:\n",
    "        for file in output_files:\n",
    "            print(f\"  - {file}\")\n",
    "        \n",
    "        print(\"\\nüì• Downloading files...\")\n",
    "        for file in output_files:\n",
    "            try:\n",
    "                files.download(f'output/{file}')\n",
    "                print(f\"‚úÖ Downloaded: {file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to download {file}: {e}\")\n",
    "    else:\n",
    "        print(\"No audio files found. Generate some audio first!\")\n",
    "else:\n",
    "    print(\"Output directory not found. Generate some audio first!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clear-memory"
   },
   "source": [
    "## üßπ Clear GPU Memory\n",
    "\n",
    "Run this cell if you encounter memory issues or want to free up GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clear-gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import gc\n",
    "\n",
    "print(\"üßπ Clearing GPU memory...\\n\")\n",
    "\n",
    "# Clear CUDA cache\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Get memory stats\n",
    "    allocated = torch.cuda.memory_allocated(0) / 1024**3\n",
    "    reserved = torch.cuda.memory_reserved(0) / 1024**3\n",
    "    \n",
    "    print(f\"GPU Memory Allocated: {allocated:.2f} GB\")\n",
    "    print(f\"GPU Memory Reserved: {reserved:.2f} GB\")\n",
    "\n",
    "# Run garbage collection\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n‚úÖ Memory cleared!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "footer"
   },
   "source": [
    "---\n",
    "\n",
    "## üìö Additional Resources\n",
    "\n",
    "- **GitHub Repository**: [Chatterbox-TTS-Extended](https://github.com/m-marie1/Chatterbox-TTS-Extended)\n",
    "- **Original Chatterbox**: [Resemble AI Chatterbox](https://github.com/resemble-ai/chatterbox)\n",
    "- **Report Issues**: [GitHub Issues](https://github.com/m-marie1/Chatterbox-TTS-Extended/issues)\n",
    "\n",
    "## ü§ù Credits\n",
    "\n",
    "- **Chatterbox-TTS-Extended**: Extended version with artifact reduction\n",
    "- **Original Chatterbox**: Resemble AI\n",
    "- **RNNoise**: Xiph.Org Foundation\n",
    "- **Whisper**: OpenAI\n",
    "\n",
    "---\n",
    "\n",
    "**Enjoy high-quality, artifact-free speech synthesis! üéâ**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
